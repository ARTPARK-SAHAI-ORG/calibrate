<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 724f74a7-f651-4d01-bf28-edd674db676d (2026-01-06 17:45Z) -->

# Open-source markdown documentation options (2026-01-06 17:45Z)

_**User (2026-01-06 17:39Z)**_

i want to also create a documentation page for the library - suggest the right open-source option to use that uses markdown and renders good documentation that is developer friendly

---

_**Agent (model composer-1, mode Agent)**_



---


Here are solid open-source options that use Markdown and are developer-friendly:

## Top recommendations

### 1. **MkDocs with Material theme**
- Markdown-native
- Python-based (fits your project)
- Material Design theme
- Easy setup and customization
- Search, navigation, and code highlighting
- GitHub Pages compatible

**Best for**: Python projects wanting quick, polished docs

### 2. **Docusaurus**
- Markdown-native
- React-based
- Versioning, i18n, search
- Modern UI/UX
- Active community

**Best for**: Larger projects needing versioning and advanced features

### 3. **VitePress**
- Markdown-native
- Vue-based, fast builds
- Simple, clean design
- Built-in search
- Minimal config

**Best for**: Fast, lightweight docs

## Recommendation

For a Python library, I’d suggest MkDocs with Material. It’s straightforward, markdown-first, and integrates well with Python projects.

Should I set up MkDocs with Material for your project? I can:
1. Add the necessary config files
2. Create a basic documentation structure
3. Set up a GitHub Actions workflow for auto-deployment (if needed)

Which option do you prefer?

---

_**User (2026-01-06 17:39Z)**_

okay use that and generate a plan for the documentation structure that will help cover the entire package features but be friendly for any developer to learn 

the goal is to make this accessible for those who want to use the framework, don't focus on how to support people who want to contribute to it.

Just focus on the users

---

_**Agent (model composer-1, mode Agent)**_



---


Reviewing the codebase to understand the package features, then drafting a user-focused documentation structure plan.


---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/aman/Documents/repos/artpark/agentloop/README.md</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pyproject.toml</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/cli.py</summary>

</details>
</tool-use>

---



---


Exploring the codebase structure to understand the features:


---

<tool-use data-tool-type="search" data-tool-name="codebase_search">
<details>
                <summary>Tool use: **codebase_search** • Searched codebase "What are the main features and capabilities of the pense framework?" • **15** results</summary>
            
| File | Lines |
|------|-------|
| `README.md` | L30 |
| `pense/agent/bot.py` | L58 |
| `pense/cli.py` | L23-311 |
| `README.md` | L27-90 |
| `pyproject.toml` | L41 |
| `pense/cli.py` | L311 |
| `README.md` | L707-725 |
| `pense/agent/bot.py` | L132-563 |
| `pense/agent/run_simulation.py` | L596-893 |
| `README.md` | L767-802 |
| `pense/llm/run_simulation.py` | L194-412 |
| `README.md` | L557-590 |
| `pense/llm/run_simulation.py` | L86 |
| `README.md` | L859-882 |
| `pense/agent/run_simulation.py` | L44-101 |

</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_dir** • Listed directory, 0 results</summary>
        
No results found
</details>
</tool-use>

---



---


Drafting a user-focused documentation structure plan for MkDocs with Material:


---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs-plan.md</summary>

**Chunk 1**
Lines added: 618, lines removed: 1

```diff
@@ -1,1 +1,618 @@
- 
+ # Pense Documentation Structure Plan
+ 
+ ## Overview
+ This plan outlines a comprehensive, user-focused documentation structure for Pense using MkDocs with Material theme. The structure is designed to help developers quickly understand and use the framework effectively.
+ 
+ ---
+ 
+ ## Documentation Structure
+ 
+ &#96;&#96;&#96;
+ docs/
+ ├── index.md                    # Landing page - What is Pense?
+ ├── getting-started/
+ │   ├── index.md               # Quick start guide
+ │   ├── installation.md        # Installation and setup
+ │   ├── configuration.md       # Environment setup and API keys
+ │   └── first-evaluation.md    # Your first evaluation (5-minute tutorial)
+ ├── concepts/
+ │   ├── index.md               # Core concepts overview
+ │   ├── evaluation-workflow.md # How evaluations work
+ │   ├── metrics.md             # Understanding metrics (WER, latency, etc.)
+ │   ├── providers.md           # Supported providers overview
+ │   └── output-structure.md    # Understanding output files
+ ├── guides/
+ │   ├── stt/
+ │   │   ├── index.md           # STT evaluation overview
+ │   │   ├── preparing-data.md  # How to prepare STT test data
+ │   │   ├── running-eval.md    # Running STT evaluations
+ │   │   ├── understanding-results.md # Interpreting STT results
+ │   │   └── leaderboard.md     # Generating STT leaderboards
+ │   ├── tts/
+ │   │   ├── index.md           # TTS evaluation overview
+ │   │   ├── preparing-data.md  # How to prepare TTS test data
+ │   │   ├── running-eval.md    # Running TTS evaluations
+ │   │   ├── understanding-results.md # Interpreting TTS results
+ │   │   └── leaderboard.md     # Generating TTS leaderboards
+ │   ├── llm/
+ │   │   ├── index.md           # LLM evaluation overview
+ │   │   ├── tests/
+ │   │   │   ├── index.md       # LLM tests overview
+ │   │   │   ├── creating-tests.md # Creating test configurations
+ │   │   │   ├── running-tests.md # Running LLM tests
+ │   │   │   ├── evaluation-types.md # Tool call vs response evaluation
+ │   │   │   └── leaderboard.md # Generating test leaderboards
+ │   │   └── simulations/
+ │   │       ├── index.md       # LLM simulations overview
+ │   │       ├── creating-config.md # Creating simulation configs
+ │   │       ├── personas-scenarios.md # Defining personas and scenarios
+ │   │       ├── evaluation-criteria.md # Setting up evaluation criteria
+ │   │       ├── running-simulations.md # Running simulations
+ │   │       └── leaderboard.md # Generating simulation leaderboards
+ │   └── agent/
+ │       ├── index.md           # Agent evaluation overview
+ │       ├── interactive-testing.md # Using the interactive agent test
+ │       ├── agent-simulations.md # Running automated agent simulations
+ │       ├── config-reference.md # Agent configuration reference
+ │       └── understanding-outputs.md # Understanding agent simulation outputs
+ ├── reference/
+ │   ├── cli-reference.md       # Complete CLI command reference
+ │   ├── config-reference.md    # Configuration file schemas
+ │   ├── providers.md           # Detailed provider information
+ │   └── metrics-reference.md   # Detailed metrics documentation
+ ├── examples/
+ │   ├── index.md               # Examples overview
+ │   ├── stt-quick-eval.md      # Quick STT evaluation example
+ │   ├── tts-quick-eval.md      # Quick TTS evaluation example
+ │   ├── llm-test-example.md    # LLM test example
+ │   ├── llm-simulation-example.md # LLM simulation example
+ │   └── agent-simulation-example.md # Agent simulation example
+ ├── troubleshooting/
+ │   ├── index.md               # Common issues and solutions
+ │   ├── api-keys.md            # API key setup issues
+ │   ├── data-format.md         # Data format errors
+ │   └── performance.md         # Performance optimization tips
+ └── faq.md                     # Frequently asked questions
+ &#96;&#96;&#96;
+ 
+ ---
+ 
+ ## Detailed Content Plan
+ 
+ ### 1. **index.md** (Landing Page)
+ **Purpose**: First impression - what Pense is and why use it
+ 
+ **Content**:
+ - Brief introduction: "An open-source simulation and testing framework for voice agents"
+ - Key value propositions:
+   - Move from manual to automated testing
+   - Evaluate components in isolation
+   - Test across multiple vendors
+   - Simulate thousands of scenarios
+ - Quick visual: What Pense does (diagram showing STT → LLM → TTS pipeline)
+ - Main use cases:
+   - Comparing STT/TTS providers
+   - Testing LLM behavior
+   - Simulating voice agent conversations
+ - Quick links to getting started
+ - Link to GitHub
+ 
+ ---
+ 
+ ### 2. **Getting Started Section**
+ 
+ #### **getting-started/index.md**
+ - Overview of the getting started journey
+ - Prerequisites (Python 3.10+, uv)
+ - What you'll learn
+ - Next steps navigation
+ 
+ #### **getting-started/installation.md**
+ - Installing uv (if needed)
+ - Installing Pense (&#96;uv sync --frozen&#96;)
+ - Verifying installation (&#96;pense --help&#96;)
+ - Common installation issues
+ 
+ #### **getting-started/configuration.md**
+ - Setting up &#96;.env&#96; file
+ - Required API keys by component
+ - Provider-specific setup:
+   - STT providers (Deepgram, Google, OpenAI, etc.)
+   - TTS providers (ElevenLabs, Cartesia, Google, etc.)
+   - LLM providers (OpenAI, OpenRouter)
+ - Testing API keys
+ - Security best practices
+ 
+ #### **getting-started/first-evaluation.md**
+ - Step-by-step tutorial: Run your first STT evaluation
+ - Using sample data
+ - Understanding the output
+ - Next steps
+ 
+ ---
+ 
+ ### 3. **Concepts Section**
+ 
+ #### **concepts/index.md**
+ - Core concepts overview
+ - How Pense fits into voice agent development
+ - Component isolation philosophy
+ - Evaluation vs simulation distinction
+ 
+ #### **concepts/evaluation-workflow.md**
+ - How evaluations work (input → processing → output)
+ - Batch processing concept
+ - Metrics collection
+ - Output file structure
+ 
+ #### **concepts/metrics.md**
+ - STT metrics: WER, string similarity, LLM judge score, latency
+ - TTS metrics: Latency (TTFB, processing time)
+ - LLM metrics: Pass/fail rates, evaluation scores
+ - What each metric means
+ - How to interpret them
+ 
+ #### **concepts/providers.md**
+ - What is a provider?
+ - Supported providers by component
+ - Provider capabilities and limitations
+ - Choosing the right provider
+ 
+ #### **concepts/output-structure.md**
+ - Common output structure
+ - File types: CSV, JSON, logs, audio files
+ - Understanding directory organization
+ - Finding specific results
+ 
+ ---
+ 
+ ### 4. **Guides Section**
+ 
+ ### **STT Guides**
+ 
+ #### **guides/stt/index.md**
+ - What is STT evaluation?
+ - When to use it
+ - Quick overview of the workflow
+ - Links to detailed guides
+ 
+ #### **guides/stt/preparing-data.md**
+ - Required directory structure
+ - CSV format (&#96;id, text&#96;)
+ - Audio formats (WAV, PCM16)
+ - Sample data structure
+ - Tips for creating good test data
+ - Common mistakes
+ 
+ #### **guides/stt/running-eval.md**
+ - Command syntax: &#96;pense stt eval&#96;
+ - All parameters explained:
+   - &#96;-p, --provider&#96;: Provider selection
+   - &#96;-l, --language&#96;: Language options
+   - &#96;-i, --input-dir&#96;: Input directory
+   - &#96;-o, --output-dir&#96;: Output directory
+   - &#96;-d, --debug&#96;: Debug mode
+ - Example commands
+ - What happens during execution
+ - Monitoring progress
+ 
+ #### **guides/stt/understanding-results.md**
+ - Output directory structure
+ - &#96;results.csv&#96;: Columns explained
+ - &#96;metrics.json&#96;: Structure and meaning
+ - &#96;logs/&#96;: How to use logs
+ - Interpreting WER scores
+ - Understanding latency metrics
+ - Common result patterns
+ 
+ #### **guides/stt/leaderboard.md**
+ - What is a leaderboard?
+ - When to use it
+ - Command: &#96;pense stt leaderboard&#96;
+ - Output files (Excel, PNG charts)
+ - Reading leaderboard results
+ - Comparing providers
+ 
+ ### **TTS Guides** (Similar structure to STT)
+ 
+ #### **guides/tts/index.md**
+ - What is TTS evaluation?
+ - When to use it
+ - Quick overview
+ 
+ #### **guides/tts/preparing-data.md**
+ - CSV format (single &#96;text&#96; column)
+ - Sample data examples
+ - Best practices
+ 
+ #### **guides/tts/running-eval.md**
+ - Command syntax and parameters
+ - Examples
+ - Execution flow
+ 
+ #### **guides/tts/understanding-results.md**
+ - Output structure
+ - Audio files location
+ - Metrics interpretation
+ - Latency analysis
+ 
+ #### **guides/tts/leaderboard.md**
+ - Generating leaderboards
+ - Comparing TTS providers
+ 
+ ### **LLM Guides**
+ 
+ #### **guides/llm/index.md**
+ - LLM evaluation overview
+ - Tests vs Simulations distinction
+ - When to use each
+ 
+ #### **guides/llm/tests/index.md**
+ - What are LLM tests?
+ - Use cases: verifying tool calls, response quality
+ - Quick overview
+ 
+ #### **guides/llm/tests/creating-tests.md**
+ - Test configuration structure
+ - System prompts
+ - Tools definition
+ - Test cases structure
+ - History format
+ - Evaluation types (tool_call vs response)
+ - Variables and templating
+ - Complete example
+ 
+ #### **guides/llm/tests/running-tests.md**
+ - Command: &#96;pense llm tests run&#96;
+ - Parameters explained
+ - Example commands
+ - Understanding output
+ 
+ #### **guides/llm/tests/evaluation-types.md**
+ - Tool call evaluation: How it works, examples
+ - Response evaluation: LLM judge, criteria, examples
+ - Choosing the right evaluation type
+ 
+ #### **guides/llm/tests/leaderboard.md**
+ - Generating test leaderboards
+ - Comparing models/scenarios
+ 
+ #### **guides/llm/simulations/index.md**
+ - What are LLM simulations?
+ - Use cases: end-to-end conversation testing
+ - Quick overview
+ 
+ #### **guides/llm/simulations/creating-config.md**
+ - Configuration structure
+ - Agent system prompt
+ - Tools definition
+ - Personas and scenarios
+ - Evaluation criteria
+ 
+ #### **guides/llm/simulations/personas-scenarios.md**
+ - What are personas?
+ - Creating effective personas
+ - What are scenarios?
+ - Creating scenarios
+ - Persona-scenario combinations
+ - Examples
+ 
+ #### **guides/llm/simulations/evaluation-criteria.md**
+ - What are evaluation criteria?
+ - Defining good criteria
+ - LLM judge evaluation
+ - Examples
+ 
+ #### **guides/llm/simulations/running-simulations.md**
+ - Command: &#96;pense llm simulations run&#96;
+ - Parameters
+ - Execution flow
+ - Understanding output
+ 
+ #### **guides/llm/simulations/leaderboard.md**
+ - Generating simulation leaderboards
+ - Aggregating results
+ 
+ ### **Agent Guides**
+ 
+ #### **guides/agent/index.md**
+ - Agent evaluation overview
+ - Interactive testing vs simulations
+ - When to use each
+ 
+ #### **guides/agent/interactive-testing.md**
+ - What is interactive testing?
+ - Use case: Manual testing with real audio
+ - Command: &#96;pense agent test&#96;
+ - Configuration file
+ - Starting the server
+ - Using the web UI
+ - Understanding metrics tab
+ - Output files
+ - Tips for effective testing
+ 
+ #### **guides/agent/agent-simulations.md**
+ - What are agent simulations?
+ - Use case: Automated voice agent testing
+ - Command: &#96;pense agent simulation&#96;
+ - Configuration structure
+ - Personas and scenarios
+ - Execution flow
+ - Understanding output
+ 
+ #### **guides/agent/config-reference.md**
+ - Complete configuration schema
+ - System prompt guidelines
+ - Tools configuration
+ - Provider configuration
+ - Language settings
+ - Advanced options
+ 
+ #### **guides/agent/understanding-outputs.md**
+ - Output directory structure
+ - Audio files (&#96;audios/&#96;)
+ - Transcripts (&#96;transcripts.json&#96;)
+ - Tool calls (&#96;tool_calls.json&#96;)
+ - STT outputs (&#96;stt_outputs.json&#96;)
+ - Metrics (&#96;metrics.json&#96;)
+ - Logs
+ - How to analyze results
+ 
+ ---
+ 
+ ### 5. **Reference Section**
+ 
+ #### **reference/cli-reference.md**
+ - Complete CLI command reference
+ - All commands and subcommands
+ - All flags and options
+ - Examples for each command
+ - Organized by component
+ 
+ #### **reference/config-reference.md**
+ - JSON schema for all config types
+ - Required vs optional fields
+ - Type definitions
+ - Validation rules
+ - Examples
+ 
+ #### **reference/providers.md**
+ - Detailed provider information
+ - STT providers: capabilities, setup, limitations
+ - TTS providers: capabilities, setup, limitations
+ - LLM providers: models, setup
+ - Provider comparison tables
+ 
+ #### **reference/metrics-reference.md**
+ - Complete metrics documentation
+ - Calculation methods
+ - Interpretation guidelines
+ - Normal ranges
+ - Troubleshooting metrics
+ 
+ ---
+ 
+ ### 6. **Examples Section**
+ 
+ #### **examples/index.md**
+ - Examples overview
+ - Quick links to all examples
+ - Prerequisites
+ 
+ #### **examples/stt-quick-eval.md**
+ - Complete walkthrough
+ - Sample data setup
+ - Running evaluation
+ - Interpreting results
+ 
+ #### **examples/tts-quick-eval.md**
+ - Complete walkthrough
+ - Sample data setup
+ - Running evaluation
+ - Interpreting results
+ 
+ #### **examples/llm-test-example.md**
+ - Complete test configuration
+ - Running tests
+ - Understanding results
+ 
+ #### **examples/llm-simulation-example.md**
+ - Complete simulation configuration
+ - Running simulation
+ - Understanding results
+ 
+ #### **examples/agent-simulation-example.md**
+ - Complete agent simulation configuration
+ - Running simulation
+ - Understanding results
+ 
+ ---
+ 
+ ### 7. **Troubleshooting Section**
+ 
+ #### **troubleshooting/index.md**
+ - Common issues overview
+ - Quick links
+ - How to get help
+ 
+ #### **troubleshooting/api-keys.md**
+ - API key not found errors
+ - Invalid API key errors
+ - Provider-specific issues
+ - Environment variable setup
+ 
+ #### **troubleshooting/data-format.md**
+ - CSV format errors
+ - Audio format issues
+ - Directory structure problems
+ - Common mistakes and fixes
+ 
+ #### **troubleshooting/performance.md**
+ - Slow evaluations
+ - Memory issues
+ - Debug mode usage
+ - Optimization tips
+ 
+ ---
+ 
+ ### 8. **FAQ**
+ - Frequently asked questions
+ - Common misconceptions
+ - Quick answers
+ - Links to detailed docs
+ 
+ ---
+ 
+ ## MkDocs Configuration Structure
+ 
+ &#96;&#96;&#96;yaml
+ # mkdocs.yml structure
+ site_name: Pense Documentation
+ site_description: Documentation for Pense - Voice Agent Testing Framework
+ site_url: https://your-site-url.com
+ repo_url: https://github.com/your-repo/pense
+ repo_name: pense
+ 
+ theme:
+   name: material
+   palette:
+     - scheme: default
+       primary: indigo
+       accent: indigo
+       toggle:
+         icon: material/brightness-7
+         name: Switch to dark mode
+     - scheme: slate
+       primary: indigo
+       accent: indigo
+       toggle:
+         icon: material/brightness-4
+         name: Switch to light mode
+   features:
+     - navigation.tabs
+     - navigation.sections
+     - navigation.expand
+     - navigation.top
+     - search.suggest
+     - search.highlight
+     - content.code.annotate
+     - content.code.copy
+ 
+ markdown_extensions:
+   - pymdownx.highlight:
+       anchor_linenums: true
+   - pymdownx.inlinehilite
+   - pymdownx.snippets
+   - pymdownx.superfences:
+       custom_fences:
+         - name: mermaid
+           class: mermaid
+           format: !!python/name:pymdownx.superfences.fence_code_format
+   - admonition
+   - pymdownx.details
+   - pymdownx.tabbed:
+       alternate_style: true
+   - tables
+   - toc:
+       permalink: true
+ 
+ nav:
+   - Home: index.md
+   - Getting Started:
+     - getting-started/index.md
+     - getting-started/installation.md
+     - getting-started/configuration.md
+     - getting-started/first-evaluation.md
+   - Concepts:
+     - concepts/index.md
+     - concepts/evaluation-workflow.md
+     - concepts/metrics.md
+     - concepts/providers.md
+     - concepts/output-structure.md
+   - Guides:
+     - STT Evaluation:
+       - guides/stt/index.md
+       - guides/stt/preparing-data.md
+       - guides/stt/running-eval.md
+       - guides/stt/understanding-results.md
+       - guides/stt/leaderboard.md
+     - TTS Evaluation:
+       - guides/tts/index.md
+       - guides/tts/preparing-data.md
+       - guides/tts/running-eval.md
+       - guides/tts/understanding-results.md
+       - guides/tts/leaderboard.md
+     - LLM Evaluation:
+       - guides/llm/index.md
+       - LLM Tests:
+         - guides/llm/tests/index.md
+         - guides/llm/tests/creating-tests.md
+         - guides/llm/tests/running-tests.md
+         - guides/llm/tests/evaluation-types.md
+         - guides/llm/tests/leaderboard.md
+       - LLM Simulations:
+         - guides/llm/simulations/index.md
+         - guides/llm/simulations/creating-config.md
+         - guides/llm/simulations/personas-scenarios.md
+         - guides/llm/simulations/evaluation-criteria.md
+         - guides/llm/simulations/running-simulations.md
+         - guides/llm/simulations/leaderboard.md
+     - Agent Evaluation:
+       - guides/agent/index.md
+       - guides/agent/interactive-testing.md
+       - guides/agent/agent-simulations.md
+       - guides/agent/config-reference.md
+       - guides/agent/understanding-outputs.md
+   - Reference:
+     - reference/cli-reference.md
+     - reference/config-reference.md
+     - reference/providers.md
+     - reference/metrics-reference.md
+   - Examples:
+     - examples/index.md
+     - examples/stt-quick-eval.md
+     - examples/tts-quick-eval.md
+     - examples/llm-test-example.md
+     - examples/llm-simulation-example.md
+     - examples/agent-simulation-example.md
+   - Troubleshooting:
+     - troubleshooting/index.md
+     - troubleshooting/api-keys.md
+     - troubleshooting/data-format.md
+     - troubleshooting/performance.md
+   - FAQ: faq.md
+ &#96;&#96;&#96;
+ 
+ ---
+ 
+ ## Key Documentation Principles
+ 
+ 1. **User-Focused**: Every page answers "How do I use this?" not "How is this implemented?"
+ 2. **Progressive Disclosure**: Start simple, add complexity gradually
+ 3. **Examples First**: Show examples before explaining concepts
+ 4. **Visual Aids**: Use diagrams, code blocks, and screenshots liberally
+ 5. **Quick Reference**: Make it easy to find specific information
+ 6. **Consistent Structure**: Similar sections follow the same pattern
+ 7. **Action-Oriented**: Focus on what users can do, not just what exists
+ 
+ ---
+ 
+ ## Content Style Guidelines
+ 
+ - **Tone**: Friendly, helpful, professional
+ - **Voice**: Second person ("you") or imperative ("Run the command...")
+ - **Code**: Always show complete, runnable examples
+ - **Structure**: Use clear headings, bullet points, and callouts
+ - **Cross-references**: Link liberally to related topics
+ - **Warnings**: Use admonitions for important notes and warnings
+ 
+ ---
+ 
+ ## Next Steps
+ 
+ 1. Set up MkDocs with Material theme
+ 2. Create the directory structure
+ 3. Write content starting with Getting Started section
+ 4. Add examples and reference documentation
+ 5. Review and iterate based on user feedback
+ 
```

</details>

</tool-use>

---



---


Created a documentation structure plan for Pense. Summary:

## Documentation Structure Overview

The plan includes:

### 1. **Landing & Getting Started** (4 pages)
- Introduction and value proposition
- Installation and setup
- Configuration and API keys
- First evaluation tutorial

### 2. **Core Concepts** (5 pages)
- Evaluation workflow
- Metrics explained
- Providers overview
- Output structure

### 3. **Component Guides** (20+ pages)
- **STT**: Data prep → Running → Understanding results → Leaderboards
- **TTS**: Same structure as STT
- **LLM Tests**: Creating tests → Running → Evaluation types → Leaderboards
- **LLM Simulations**: Config → Personas/Scenarios → Criteria → Running → Leaderboards
- **Agent**: Interactive testing → Simulations → Config reference → Understanding outputs

### 4. **Reference** (4 pages)
- Complete CLI reference
- Configuration schemas
- Provider details
- Metrics reference

### 5. **Examples** (6 pages)
- Quick-start examples for each component

### 6. **Troubleshooting** (4 pages)
- Common issues and solutions

### 7. **FAQ**
- Quick answers to common questions

## Features

- User-focused: "How do I use this?" not "How is this implemented?"
- Progressive: Simple → Advanced
- Example-driven: Working examples throughout
- Consistent structure: Similar patterns across components
- Quick reference: Easy to find specific information

The plan includes:
- Complete directory structure
- Detailed content outline for each page
- MkDocs Material theme configuration
- Documentation principles and style guidelines

Saved as `docs-plan.md` in your project root. Should I:
1. Set up the MkDocs structure and initial files?
2. Start writing content for specific sections?
3. Create the MkDocs configuration file?

---

_**User (2026-01-06 17:39Z)**_

how will you deploy this?

---

_**Agent (model composer-1, mode Plan)**_



---


Checking the repository for existing deployment or CI/CD configuration:


---

<tool-use data-tool-type="generic" data-tool-name="glob_file_search">
<details>
<summary>Tool use: **glob_file_search** • Searched codebase "*.yml" • **0** result in **1** directory</summary>

Directory: **/Users/aman/Documents/repos/artpark/agentloop** (0 file)

</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="glob_file_search">
<details>
<summary>Tool use: **glob_file_search** • Searched codebase "*.yaml" • **0** result in **1** directory</summary>

Directory: **/Users/aman/Documents/repos/artpark/agentloop** (0 file)

</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_dir** • Listed directory, 0 results</summary>
        
No results found
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pyproject.toml</summary>

</details>
</tool-use>

---



---


Asking a few questions to determine the best deployment approach:


---

<tool-use data-tool-type="unknown" data-tool-name="ask_question">
<details>
<summary>Tool use: **ask_question**</summary>


Parameters:

```json
{
  "questions": [
    {
      "id": "deployment_platform",
      "prompt": "Which deployment platform would you prefer for the documentation?",
      "options": [
        {
          "id": "github_pages",
          "label": "GitHub Pages (free, automatic, integrates with GitHub repos)"
        },
        {
          "id": "netlify",
          "label": "Netlify (free tier, easy setup, custom domain support)"
        },
        {
          "id": "vercel",
          "label": "Vercel (free tier, fast, good for static sites)"
        },
        {
          "id": "other",
          "label": "Other (please specify)"
        }
      ]
    },
    {
      "id": "auto_deploy",
      "prompt": "Do you want automatic deployment when changes are pushed to the repository?",
      "options": [
        {
          "id": "yes",
          "label": "Yes, deploy automatically on push to main/master"
        },
        {
          "id": "no",
          "label": "No, manual deployment is fine"
        }
      ]
    }
  ]
}
```
Additional data:

```json
{
  "status": "submitted",
  "currentSelections": {}
}
```
Result:

```json
{
  "answers": [
    {
      "questionId": "deployment_platform"
    },
    {
      "questionId": "auto_deploy"
    }
  ]
}
```
User decision: **accepted**

Status: **completed**


</details>
</tool-use>

---

_**User (2026-01-06 17:39Z)**_

github pages?

---

_**Agent (model composer-1, mode Plan)**_



---


Creating a plan to deploy the MkDocs documentation to GitHub Pages with automatic deployment.


---

<tool-use data-tool-type="unknown" data-tool-name="create_plan">
<details>
<summary>Tool use: **create_plan**</summary>


Parameters:

```json
{
  "plan": "# Deploy MkDocs Documentation to GitHub Pages\n\n## Overview\nSet up MkDocs with Material theme and configure automatic deployment to GitHub Pages via GitHub Actions. The documentation will rebuild and deploy automatically whenever changes are pushed to the main branch.\n\n## Implementation Plan\n\n### 1. Project Structure Setup\n\nCreate the documentation directory structure:\n- &#96;docs/&#96; - Main documentation source directory (markdown files)\n- &#96;mkdocs.yml&#96; - MkDocs configuration file\n- &#96;.github/workflows/docs.yml&#96; - GitHub Actions workflow for deployment\n\n### 2. MkDocs Configuration\n\nCreate &#96;mkdocs.yml&#96; with:\n- Material theme configuration\n- Navigation structure matching the documentation plan\n- Markdown extensions (code highlighting, admonitions, tables)\n- Site metadata (name, description, repo URL)\n- Theme customization (colors, features)\n\n### 3. GitHub Actions Workflow\n\nCreate &#96;.github/workflows/docs.yml&#96; that:\n- Triggers on push to main branch and on pull requests\n- Sets up Python environment\n- Installs dependencies (mkdocs, mkdocs-material, and plugins)\n- Builds the documentation site\n- Deploys to GitHub Pages using &#96;mkdocs gh-deploy&#96; or the &#96;peaceiris/actions-gh-pages&#96; action\n- Uses GitHub token for authentication\n\n### 4. Dependencies\n\nAdd documentation dependencies to &#96;pyproject.toml&#96;:\n- &#96;mkdocs&gt;=1.5.0&#96;\n- &#96;mkdocs-material&gt;=9.0.0&#96;\n- &#96;mkdocs-material-extensions&#96; (if needed)\n- Optional: &#96;mkdocs-mermaid2-plugin&#96; for diagrams\n\nOr create a separate &#96;requirements-docs.txt&#96; for documentation-only dependencies.\n\n### 5. Initial Documentation Files\n\nCreate initial markdown files:\n- &#96;docs/index.md&#96; - Landing page\n- &#96;docs/getting-started/index.md&#96; - Getting started overview\n- Basic structure to validate the setup\n\n### 6. GitHub Pages Configuration\n\nConfigure GitHub repository settings:\n- Enable GitHub Pages\n- Set source to \"GitHub Actions\" (not branch)\n- Optionally configure custom domain\n\n### 7. Documentation Workflow\n\nThe workflow will:\n1. Checkout repository\n2. Set up Python (using uv or standard Python)\n3. Install MkDocs and dependencies\n4. Build documentation (&#96;mkdocs build&#96;)\n5. Deploy to &#96;gh-pages&#96; branch\n\n## Files to Create/Modify\n\n### New Files:\n- &#96;mkdocs.yml&#96; - Main MkDocs configuration\n- &#96;.github/workflows/docs.yml&#96; - GitHub Actions workflow\n- &#96;docs/index.md&#96; - Landing page\n- &#96;docs/getting-started/index.md&#96; - Getting started page\n- &#96;requirements-docs.txt&#96; - Documentation dependencies (optional, or add to pyproject.toml)\n\n### Modified Files:\n- &#96;pyproject.toml&#96; - Add documentation dependencies (optional)\n- &#96;.gitignore&#96; - Add &#96;site/&#96; directory (MkDocs build output)\n\n## Deployment Flow\n\n&#96;&#96;&#96;mermaid\ngraph LR\n    A[Push to main] --&gt; B[GitHub Actions Triggered]\n    B --&gt; C[Checkout Code]\n    C --&gt; D[Install Dependencies]\n    D --&gt; E[Build MkDocs Site]\n    E --&gt; F[Deploy to gh-pages]\n    F --&gt; G[GitHub Pages Serves Site]\n&#96;&#96;&#96;\n\n## Key Configuration Details\n\n### GitHub Actions Workflow Features:\n- Runs on: &#96;push&#96; to &#96;main&#96; branch and &#96;pull_request&#96; events\n- Python version: 3.10+ (matching project requirements)\n- Build command: &#96;mkdocs build&#96;\n- Deploy method: &#96;mkdocs gh-deploy --force&#96; or &#96;peaceiris/actions-gh-pages&#96;\n- Permissions: &#96;contents: write&#96; for deployment\n\n### MkDocs Configuration Features:\n- Material theme with light/dark mode toggle\n- Navigation tabs and sections\n- Search functionality\n- Code highlighting with copy buttons\n- Responsive design\n\n## Benefits\n\n1. **Automatic Deployment**: Documentation updates automatically on every push\n2. **Version Control**: Documentation changes tracked in git\n3. **Free Hosting**: GitHub Pages is free for public repositories\n4. **Custom Domain Support**: Can use custom domain if needed\n5. **Preview PRs**: Can preview documentation changes in pull requests\n\n## Next Steps After Deployment\n\n1. Verify site is accessible at &#96;https://&lt;username&gt;.github.io/&lt;repo-name&gt;/&#96;\n2. Test automatic deployment by making a small change\n3. Add custom domain (optional)\n4. Continue writing documentation content following the plan structure",
  "name": "Deploy MkDocs to GitHub Pages",
  "overview": "Set up MkDocs documentation with Material theme and deploy it to GitHub Pages using GitHub Actions for automatic deployment on every push to main branch."
}
```
Additional data:

```json
{
  "reviewData": {
    "status": "Requested",
    "selectedOption": "none",
    "isShowingInput": false
  },
  "planUri": "file:///Users/aman/.cursor/plans/deploy_mkdocs_to_github_pages_224e5994.plan.md",
  "hasOpenedEditor": true,
  "pinnedName": "Deploy MkDocs to GitHub Pages"
}
```
Result:

```json
{
  "accepted": {},
  "planUri": "/Users/aman/.cursor/plans/deploy_mkdocs_to_github_pages_224e5994.plan.md"
}
```
Status: **completed**


</details>
</tool-use>

---

_**User (2026-01-06 17:39Z)**_

my repo is private and i need to use free options what to do

---

_**Agent (model composer-1, mode Plan)**_



---


Updating the plan to address private repository hosting. GitHub Pages on private repos has limitations; outlining free alternatives:


---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/aman/Documents/repos/artpark/agentloop/.gitignore</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="search" data-tool-name="web_search">
<details>
            <summary>Tool use: **web_search** • Searched web `free documentation hosting private repository 2024 netlify vercel cloudflare pages` • **1** results</summary>
        
| Title | URL | Chunk |
|-------|-----|-------|
| Web Search Results |  | Deploying your MkDocs documentation from a private GitHub repository using free options can be achieved by leveraging GitHub Actions and GitHub Pages. Here's a step-by-step guide:<br/><br/>1. **Set Up Your Documentation with MkDocs**:<br/>   - Ensure your project includes an `mkdocs.yml` configuration file and a `docs/` directory containing your Markdown files.<br/><br/>2. **Configure GitHub Pages**:<br/>   - Navigate to your repository's settings on GitHub.<br/>   - Under the "Pages" section, set the source to the `gh-pages` branch.<br/><br/>3. **Create a GitHub Actions Workflow**:<br/>   - In your repository, create a directory named `.github/workflows/`.<br/>   - Inside this directory, create a file named `deploy.yml` with the following content:<br/><br/>     ```yaml<br/>     name: Deploy MkDocs to GitHub Pages<br/><br/>     on:<br/>       push:<br/>         branches:<br/>           - main  # Adjust if your default branch is named differently<br/><br/>     permissions:<br/>       contents: write<br/><br/>     jobs:<br/>       deploy:<br/>         runs-on: ubuntu-latest<br/>         steps:<br/>           - name: Checkout repository<br/>             uses: actions/checkout@v2<br/><br/>           - name: Set up Python<br/>             uses: actions/setup-python@v2<br/>             with:<br/>               python-version: '3.x'<br/><br/>           - name: Install MkDocs and Material theme<br/>             run: \|<br/>               pip install mkdocs<br/>               pip install mkdocs-material<br/><br/>           - name: Build documentation<br/>             run: mkdocs build<br/><br/>           - name: Deploy to GitHub Pages<br/>             uses: peaceiris/actions-gh-pages@v3<br/>             with:<br/>               github_token: $\{{ secrets.GITHUB_TOKEN \}}<br/>               publish_dir: ./site<br/>     ```<br/><br/>     This workflow will trigger on every push to the `main` branch, build your MkDocs documentation, and deploy it to the `gh-pages` branch using GitHub Pages.<br/><br/>4. **Commit and Push Changes**:<br/>   - Add, commit, and push the `.github/workflows/deploy.yml` file to your repository.<br/><br/>5. **Access Your Documentation**:<br/>   - Once the workflow runs successfully, your documentation will be available at `https://<username>.github.io/<repository>/`.<br/><br/>**Note**: Ensure that your repository's settings allow GitHub Actions to write to the repository. This method utilizes GitHub's free offerings, making it suitable for private repositories without incurring additional costs.<br/><br/>For more detailed information, refer to the [MkDocs documentation on deploying your docs](https://www.mkdocs.org/user-guide/deploying-your-docs/). |

</details>
</tool-use>

---



---


Updating the plan with free options for private repositories:


---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/aman/.cursor/plans/deploy_mkdocs_to_github_pages_224e5994.plan.md</summary>

</details>
</tool-use>

---



---





---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: /Users/aman/.cursor/plans/deploy_mkdocs_to_github_pages_224e5994.plan.md</summary>

**Chunk 1**
Lines added: 17, lines removed: 4

```diff
@@ -1,13 +1,26 @@
  ---
- name: Deploy MkDocs to GitHub Pages
- overview: Set up MkDocs documentation with Material theme and deploy it to GitHub Pages using GitHub Actions for automatic deployment on every push to main branch.
+ name: Deploy MkDocs Documentation (Private Repo Compatible)
+ overview: Set up MkDocs documentation with Material theme and deploy it using free hosting options that work with private repositories. Provides multiple deployment options: Netlify (recommended), GitHub Pages, Vercel, and Cloudflare Pages.
  todos: []
  ---
  
- # Deploy MkDocs Documentation to GitHub Pages
+ # Deploy MkDocs Documentation (Private Repository Compatible)
  
  ## Overview
  
- Set up MkDocs with Material theme and configure automatic deployment to GitHub Pages via GitHub Actions. The documentation will rebuild and deploy automatically whenever changes are pushed to the main branch.
+ Set up MkDocs with Material theme and configure automatic deployment using free hosting services that support private repositories. The documentation will rebuild and deploy automatically whenever changes are pushed to the main branch.
+ 
+ ## Private Repository Considerations
+ 
+ **GitHub Pages Limitations:**
+ - Personal accounts: Free for up to 3 private repositories with GitHub Pages
+ - Organization accounts: Requires GitHub Pro/Team plan for private repo Pages
+ - If you exceed limits or need organization support, use alternative options below
+ 
+ **Recommended Free Alternatives:**
+ 1. **Netlify** (Recommended) - Free tier, unlimited private repos, easy setup
+ 2. **Vercel** - Free tier, unlimited private repos, fast CDN
+ 3. **Cloudflare Pages** - Free tier, unlimited private repos, global CDN
+ 4. **GitHub Pages** - Free for personal accounts (up to 3 private repos)
  
  ## Implementation Plan
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: /Users/aman/.cursor/plans/deploy_mkdocs_to_github_pages_224e5994.plan.md</summary>

**Chunk 1**
Lines added: 110, lines removed: 27

```diff
@@ -28,32 +28,115 @@
  ## Implementation Plan
  
- ### 1. Project Structure Setup
- 
- Create the documentation directory structure:
- 
- - &#96;docs/&#96; - Main documentation source directory (markdown files)
- - &#96;mkdocs.yml&#96; - MkDocs configuration file
- - &#96;.github/workflows/docs.yml&#96; - GitHub Actions workflow for deployment
- 
- ### 2. MkDocs Configuration
- 
- Create &#96;mkdocs.yml&#96; with:
- 
- - Material theme configuration
- - Navigation structure matching the documentation plan
- - Markdown extensions (code highlighting, admonitions, tables)
- - Site metadata (name, description, repo URL)
- - Theme customization (colors, features)
+ ### Option 1: Netlify (Recommended for Private Repos)
+ 
+ **Why Netlify:**
+ - Free tier supports unlimited private repositories
+ - Easy GitHub integration
+ - Automatic deployments on push
+ - Preview deployments for pull requests
+ - Custom domain support
+ - No credit card required
+ 
+ **Setup Steps:**
+ 
+ 1. **Project Structure:**
+    - &#96;docs/&#96; - Main documentation source directory (markdown files)
+    - &#96;mkdocs.yml&#96; - MkDocs configuration file
+    - &#96;netlify.toml&#96; - Netlify build configuration (optional)
+ 
+ 2. **MkDocs Configuration:**
+    - Create &#96;mkdocs.yml&#96; with Material theme
+    - Navigation structure matching the documentation plan
+    - Markdown extensions (code highlighting, admonitions, tables)
+    - Site metadata (name, description, repo URL)
+    - Theme customization (colors, features)
  
- ### 3. GitHub Actions Workflow
- 
- Create &#96;.github/workflows/docs.yml&#96; that:
- 
- - Triggers on push to main branch and on pull requests
- - Sets up Python environment
- - Installs dependencies (mkdocs, mkdocs-material, and plugins)
- - Builds the documentation site
- - Deploys to GitHub Pages using &#96;mkdocs gh-deploy&#96; or the &#96;peaceiris/actions-gh-pages&#96; action
- - Uses GitHub token for authentication
+ 3. **Netlify Configuration:**
+    - Create &#96;netlify.toml&#96; with build settings:
+      &#96;&#96;&#96;toml
+      [build]
+        command = "pip install -r requirements-docs.txt &amp;&amp; mkdocs build"
+        publish = "site"
+      &#96;&#96;&#96;
+    - Or configure via Netlify UI
+ 
+ 4. **Deployment:**
+    - Connect GitHub repository to Netlify
+    - Netlify auto-detects MkDocs or use manual configuration
+    - Build command: &#96;pip install -r requirements-docs.txt &amp;&amp; mkdocs build&#96;
+    - Publish directory: &#96;site&#96;
+    - Automatic deployments on every push
+ 
+ ### Option 2: GitHub Pages (If Available)
+ 
+ **Setup Steps:**
+ 
+ 1. **Project Structure:**
+    - &#96;docs/&#96; - Main documentation source directory
+    - &#96;mkdocs.yml&#96; - MkDocs configuration file
+    - &#96;.github/workflows/docs.yml&#96; - GitHub Actions workflow
+ 
+ 2. **MkDocs Configuration:**
+    - Same as Option 1
+ 
+ 3. **GitHub Actions Workflow:**
+    - Create &#96;.github/workflows/docs.yml&#96; that:
+      - Triggers on push to main branch
+      - Sets up Python environment
+      - Installs dependencies (mkdocs, mkdocs-material)
+      - Builds documentation (&#96;mkdocs build&#96;)
+      - Deploys using &#96;peaceiris/actions-gh-pages&#96; action
+      - Uses &#96;GITHUB_TOKEN&#96; for authentication
+ 
+ 4. **GitHub Pages Settings:**
+    - Enable GitHub Pages in repository settings
+    - Set source to "GitHub Actions"
+    - Site will be available at &#96;https://&lt;username&gt;.github.io/&lt;repo-name&gt;/&#96;
+ 
+ ### Option 3: Vercel
+ 
+ **Why Vercel:**
+ - Free tier supports unlimited private repos
+ - Fast global CDN
+ - Automatic deployments
+ - Preview deployments for PRs
+ 
+ **Setup Steps:**
+ 
+ 1. **Project Structure:**
+    - Same as Option 1
+ 
+ 2. **Vercel Configuration:**
+    - Create &#96;vercel.json&#96; (optional):
+      &#96;&#96;&#96;json
+      {
+        "buildCommand": "pip install -r requirements-docs.txt &amp;&amp; mkdocs build",
+        "outputDirectory": "site"
+      }
+      &#96;&#96;&#96;
+    - Or configure via Vercel UI
+ 
+ 3. **Deployment:**
+    - Connect GitHub repository to Vercel
+    - Configure build settings
+    - Automatic deployments on push
+ 
+ ### Option 4: Cloudflare Pages
+ 
+ **Why Cloudflare Pages:**
+ - Free tier supports unlimited private repos
+ - Global CDN
+ - Fast builds
+ 
+ **Setup Steps:**
+ 
+ 1. **Project Structure:**
+    - Same as Option 1
+ 
+ 2. **Cloudflare Configuration:**
+    - Connect GitHub repository
+    - Build command: &#96;pip install -r requirements-docs.txt &amp;&amp; mkdocs build&#96;
+    - Build output directory: &#96;site&#96;
+    - Python version: 3.10+
  
  ### 4. Dependencies
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: /Users/aman/.cursor/plans/deploy_mkdocs_to_github_pages_224e5994.plan.md</summary>

**Chunk 1**
Lines added: 12, lines removed: 7

```diff
@@ -142,12 +142,17 @@
  ### 4. Dependencies
  
- Add documentation dependencies to &#96;pyproject.toml&#96;:
+ Create &#96;requirements-docs.txt&#96; for documentation dependencies (recommended to keep separate from main project):
  
- - &#96;mkdocs&gt;=1.5.0&#96;
- - &#96;mkdocs-material&gt;=9.0.0&#96;
- - &#96;mkdocs-material-extensions&#96; (if needed)
- - Optional: &#96;mkdocs-mermaid2-plugin&#96; for diagrams
- 
- Or create a separate &#96;requirements-docs.txt&#96; for documentation-only dependencies.
+ &#96;&#96;&#96;
+ mkdocs&gt;=1.5.0
+ mkdocs-material&gt;=9.0.0
+ mkdocs-material-extensions
+ &#96;&#96;&#96;
+ 
+ Optional plugins:
+ - &#96;mkdocs-mermaid2-plugin&#96; - For Mermaid diagrams
+ - &#96;mkdocs-minify-plugin&#96; - Minify HTML/CSS/JS
+ 
+ **Note:** Keep documentation dependencies separate from main project dependencies for cleaner builds.
  
  ### 5. Initial Documentation Files
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: /Users/aman/.cursor/plans/deploy_mkdocs_to_github_pages_224e5994.plan.md</summary>

**Chunk 1**
Lines added: 13, lines removed: 17

```diff
@@ -164,21 +164,17 @@
  - Basic structure to validate the setup
  
- ### 6. GitHub Pages Configuration
- 
- Configure GitHub repository settings:
- 
- - Enable GitHub Pages
- - Set source to "GitHub Actions" (not branch)
- - Optionally configure custom domain
- 
- ### 7. Documentation Workflow
- 
- The workflow will:
- 
- 1. Checkout repository
- 2. Set up Python (using uv or standard Python)
- 3. Install MkDocs and dependencies
- 4. Build documentation (&#96;mkdocs build&#96;)
- 5. Deploy to &#96;gh-pages&#96; branch
+ ### 6. Deployment Workflow
+ 
+ **For Netlify/Vercel/Cloudflare Pages:**
+ 1. Connect repository to hosting service
+ 2. Configure build settings
+ 3. Automatic deployment on push to main branch
+ 4. Preview deployments for pull requests
+ 
+ **For GitHub Pages:**
+ 1. GitHub Actions workflow runs on push
+ 2. Builds documentation
+ 3. Deploys to &#96;gh-pages&#96; branch
+ 4. GitHub Pages serves from &#96;gh-pages&#96; branch
  
  ## Files to Create/Modify
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: /Users/aman/.cursor/plans/deploy_mkdocs_to_github_pages_224e5994.plan.md</summary>

**Chunk 1**
Lines added: 16, lines removed: 4

```diff
@@ -203,15 +203,27 @@
  ## Files to Create/Modify
  
- ### New Files:
+ ### New Files (All Options):
  
  - &#96;mkdocs.yml&#96; - Main MkDocs configuration
- - &#96;.github/workflows/docs.yml&#96; - GitHub Actions workflow
+ - &#96;requirements-docs.txt&#96; - Documentation dependencies
  - &#96;docs/index.md&#96; - Landing page
  - &#96;docs/getting-started/index.md&#96; - Getting started page
- - &#96;requirements-docs.txt&#96; - Documentation dependencies (optional, or add to pyproject.toml)
+ 
+ ### Option-Specific Files:
+ 
+ **For Netlify:**
+ - &#96;netlify.toml&#96; - Netlify build configuration (optional)
+ 
+ **For GitHub Pages:**
+ - &#96;.github/workflows/docs.yml&#96; - GitHub Actions workflow
+ 
+ **For Vercel:**
+ - &#96;vercel.json&#96; - Vercel configuration (optional)
+ 
+ **For Cloudflare Pages:**
+ - No additional files needed (configure via UI)
  
  ### Modified Files:
  
- - &#96;pyproject.toml&#96; - Add documentation dependencies (optional)
  - &#96;.gitignore&#96; - Add &#96;site/&#96; directory (MkDocs build output)
  
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: /Users/aman/.cursor/plans/deploy_mkdocs_to_github_pages_224e5994.plan.md</summary>

**Chunk 1**
Lines added: 11, lines removed: undefined

```diff
@@ -230,4 +230,15 @@
  ## Deployment Flow
  
+ ### Netlify/Vercel/Cloudflare Pages Flow:
+ &#96;&#96;&#96;mermaid
+ graph LR
+     A[Push to main] --&gt; B[Hosting Service Detects Push]
+     B --&gt; C[Install Python Dependencies]
+     C --&gt; D[Build MkDocs Site]
+     D --&gt; E[Deploy to CDN]
+     E --&gt; F[Site Live]
+ &#96;&#96;&#96;
+ 
+ ### GitHub Pages Flow:
  &#96;&#96;&#96;mermaid
  graph LR
```

**Chunk 2**
Lines added: 1, lines removed: 1

```diff
@@ -236,5 +247,5 @@
      C --&gt; D[Install Dependencies]
      D --&gt; E[Build MkDocs Site]
-     E --&gt; F[Deploy to gh-pages]
+     E --&gt; F[Deploy to gh-pages Branch]
      F --&gt; G[GitHub Pages Serves Site]
  &#96;&#96;&#96;
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: /Users/aman/.cursor/plans/deploy_mkdocs_to_github_pages_224e5994.plan.md</summary>

**Chunk 1**
Lines added: 29, lines removed: 6

```diff
@@ -261,11 +261,34 @@
  ## Key Configuration Details
  
- ### GitHub Actions Workflow Features:
- 
- - Runs on: &#96;push&#96; to &#96;main&#96; branch and &#96;pull_request&#96; events
- - Python version: 3.10+ (matching project requirements)
- - Build command: &#96;mkdocs build&#96;
- - Deploy method: &#96;mkdocs gh-deploy --force&#96; or &#96;peaceiris/actions-gh-pages&#96;
+ ### Build Configuration (All Options):
+ 
+ - **Python version**: 3.10+ (matching project requirements)
+ - **Build command**: &#96;pip install -r requirements-docs.txt &amp;&amp; mkdocs build&#96;
+ - **Output directory**: &#96;site/&#96;
+ - **Dependencies**: Listed in &#96;requirements-docs.txt&#96;
+ 
+ ### Platform-Specific Details:
+ 
+ **Netlify:**
+ - Build command: &#96;pip install -r requirements-docs.txt &amp;&amp; mkdocs build&#96;
+ - Publish directory: &#96;site&#96;
+ - Python version: Auto-detected or specify in &#96;netlify.toml&#96;
+ - Environment: Ubuntu-based build environment
+ 
+ **GitHub Pages (via Actions):**
+ - Workflow triggers: &#96;push&#96; to &#96;main&#96; branch
+ - Python version: 3.10+ (specify in workflow)
+ - Deploy method: &#96;peaceiris/actions-gh-pages&#96; action
  - Permissions: &#96;contents: write&#96; for deployment
+ 
+ **Vercel:**
+ - Build command: &#96;pip install -r requirements-docs.txt &amp;&amp; mkdocs build&#96;
+ - Output directory: &#96;site&#96;
+ - Python version: Auto-detected or specify in settings
+ 
+ **Cloudflare Pages:**
+ - Build command: &#96;pip install -r requirements-docs.txt &amp;&amp; mkdocs build&#96;
+ - Build output directory: &#96;site&#96;
+ - Python version: Specify in build settings (3.10+)
  
  ### MkDocs Configuration Features:
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: /Users/aman/.cursor/plans/deploy_mkdocs_to_github_pages_224e5994.plan.md</summary>

**Chunk 1**
Lines added: 36, lines removed: 7

```diff
@@ -308,13 +308,42 @@
  1. **Automatic Deployment**: Documentation updates automatically on every push
  2. **Version Control**: Documentation changes tracked in git
- 3. **Free Hosting**: GitHub Pages is free for public repositories
- 4. **Custom Domain Support**: Can use custom domain if needed
- 5. **Preview PRs**: Can preview documentation changes in pull requests
+ 3. **Free Hosting**: All options are free for private repositories
+ 4. **Custom Domain Support**: All platforms support custom domains
+ 5. **Preview PRs**: Preview documentation changes in pull requests (Netlify/Vercel/Cloudflare)
+ 6. **No Credit Card Required**: All options are free without payment info
+ 
+ ## Recommendation
+ 
+ **For Private Repositories: Use Netlify**
+ 
+ **Reasons:**
+ - Easiest setup (just connect GitHub repo)
+ - Unlimited private repos on free tier
+ - Best documentation and community support
+ - Preview deployments for PRs
+ - No GitHub Actions workflow needed
+ - Fast and reliable
+ 
+ **Alternative:** Use GitHub Pages if you have a personal account and fewer than 3 private repos with Pages enabled.
  
  ## Next Steps After Deployment
  
- 1. Verify site is accessible at &#96;https://&lt;username&gt;.github.io/&lt;repo-name&gt;/&#96;
- 2. Test automatic deployment by making a small change
- 3. Add custom domain (optional)
- 4. Continue writing documentation content following the plan structure
+ 1. **Choose your platform** (recommended: Netlify)
+ 2. **Set up MkDocs configuration** (&#96;mkdocs.yml&#96;)
+ 3. **Create initial documentation files** (&#96;docs/index.md&#96;)
+ 4. **Connect repository** to chosen hosting platform
+ 5. **Verify site is accessible** at the provided URL
+ 6. **Test automatic deployment** by making a small change
+ 7. **Add custom domain** (optional)
+ 8. **Continue writing documentation** content following the plan structure
+ 
+ ## Quick Start Commands
+ 
+ After setup, documentation will deploy automatically. To build locally:
+ 
+ &#96;&#96;&#96;bash
+ pip install -r requirements-docs.txt
+ mkdocs serve  # Preview locally at http://127.0.0.1:8000
+ mkdocs build  # Build static site in site/ directory
+ &#96;&#96;&#96;
  
```

</details>

</tool-use>

---



---


Updated the plan to cover free hosting options for private repositories. Summary:

## Free options for private repositories

### Recommended: Netlify
- Free tier supports unlimited private repos
- Simple setup: connect your GitHub repo
- Automatic deployments on push
- Preview deployments for pull requests
- No credit card required

### Alternative options

1. GitHub Pages
   - Personal accounts: free for up to 3 private repos
   - Organization accounts: requires GitHub Pro/Team
   - Uses GitHub Actions for deployment

2. Vercel
   - Free tier supports unlimited private repos
   - Fast global CDN
   - Easy GitHub integration

3. Cloudflare Pages
   - Free tier supports unlimited private repos
   - Global CDN
   - Good performance

## Recommendation

Use Netlify for private repos:
- Easiest setup (connect repo, configure build)
- No GitHub Actions workflow needed
- Preview deployments for PRs
- Reliable and fast

The updated plan includes:
- Setup instructions for all 4 options
- Configuration files for each platform
- Build commands and settings
- Step-by-step deployment guides

Should I proceed with implementing the Netlify setup, or do you prefer a different option?

---

_**User (2026-01-06 17:39Z)**_

yep

---

_**Agent (model composer-1, mode Agent)**_



---


Implementing the Netlify setup for MkDocs documentation. Creating the necessary files:


---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/aman/Documents/repos/artpark/agentloop/docs-plan.md</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/aman/Documents/repos/artpark/agentloop/.gitignore</summary>

</details>
</tool-use>

---



---


Creating the necessary files for MkDocs with Netlify deployment:


---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/mkdocs.yml</summary>

**Chunk 1**
Lines added: 115, lines removed: 1

```diff
@@ -1,1 +1,115 @@
- 
+ site_name: Pense Documentation
+ site_description: Documentation for Pense - Voice Agent Testing Framework
+ site_url: https://pense-docs.netlify.app
+ repo_url: https://github.com/your-username/agentloop
+ repo_name: pense
+ 
+ theme:
+   name: material
+   palette:
+     - scheme: default
+       primary: indigo
+       accent: indigo
+       toggle:
+         icon: material/brightness-7
+         name: Switch to dark mode
+     - scheme: slate
+       primary: indigo
+       accent: indigo
+       toggle:
+         icon: material/brightness-4
+         name: Switch to light mode
+   features:
+     - navigation.tabs
+     - navigation.sections
+     - navigation.expand
+     - navigation.top
+     - search.suggest
+     - search.highlight
+     - content.code.annotate
+     - content.code.copy
+ 
+ markdown_extensions:
+   - pymdownx.highlight:
+       anchor_linenums: true
+   - pymdownx.inlinehilite
+   - pymdownx.snippets
+   - pymdownx.superfences:
+       custom_fences:
+         - name: mermaid
+           class: mermaid
+           format: !!python/name:pymdownx.superfences.fence_code_format
+   - admonition
+   - pymdownx.details
+   - pymdownx.tabbed:
+       alternate_style: true
+   - tables
+   - toc:
+       permalink: true
+ 
+ nav:
+   - Home: index.md
+   - Getting Started:
+     - getting-started/index.md
+     - getting-started/installation.md
+     - getting-started/configuration.md
+     - getting-started/first-evaluation.md
+   - Concepts:
+     - concepts/index.md
+     - concepts/evaluation-workflow.md
+     - concepts/metrics.md
+     - concepts/providers.md
+     - concepts/output-structure.md
+   - Guides:
+     - STT Evaluation:
+       - guides/stt/index.md
+       - guides/stt/preparing-data.md
+       - guides/stt/running-eval.md
+       - guides/stt/understanding-results.md
+       - guides/stt/leaderboard.md
+     - TTS Evaluation:
+       - guides/tts/index.md
+       - guides/tts/preparing-data.md
+       - guides/tts/running-eval.md
+       - guides/tts/understanding-results.md
+       - guides/tts/leaderboard.md
+     - LLM Evaluation:
+       - guides/llm/index.md
+       - LLM Tests:
+         - guides/llm/tests/index.md
+         - guides/llm/tests/creating-tests.md
+         - guides/llm/tests/running-tests.md
+         - guides/llm/tests/evaluation-types.md
+         - guides/llm/tests/leaderboard.md
+       - LLM Simulations:
+         - guides/llm/simulations/index.md
+         - guides/llm/simulations/creating-config.md
+         - guides/llm/simulations/personas-scenarios.md
+         - guides/llm/simulations/evaluation-criteria.md
+         - guides/llm/simulations/running-simulations.md
+         - guides/llm/simulations/leaderboard.md
+     - Agent Evaluation:
+       - guides/agent/index.md
+       - guides/agent/interactive-testing.md
+       - guides/agent/agent-simulations.md
+       - guides/agent/config-reference.md
+       - guides/agent/understanding-outputs.md
+   - Reference:
+     - reference/cli-reference.md
+     - reference/config-reference.md
+     - reference/providers.md
+     - reference/metrics-reference.md
+   - Examples:
+     - examples/index.md
+     - examples/stt-quick-eval.md
+     - examples/tts-quick-eval.md
+     - examples/llm-test-example.md
+     - examples/llm-simulation-example.md
+     - examples/agent-simulation-example.md
+   - Troubleshooting:
+     - troubleshooting/index.md
+     - troubleshooting/api-keys.md
+     - troubleshooting/data-format.md
+     - troubleshooting/performance.md
+   - FAQ: faq.md
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/requirements-docs.txt</summary>

**Chunk 1**
Lines added: 4, lines removed: 1

```diff
@@ -1,1 +1,4 @@
- 
+ mkdocs&gt;=1.5.0
+ mkdocs-material&gt;=9.0.0
+ mkdocs-material-extensions
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/netlify.toml</summary>

**Chunk 1**
Lines added: 7, lines removed: 1

```diff
@@ -1,1 +1,7 @@
- 
+ [build]
+   command = "pip install -r requirements-docs.txt &amp;&amp; mkdocs build"
+   publish = "site"
+ 
+ [build.environment]
+   PYTHON_VERSION = "3.10"
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/.gitignore</summary>

**Chunk 1**
Lines added: 3, lines removed: undefined

```diff
@@ -135,2 +135,5 @@
  out
  data
+ 
+ # MkDocs build output
+ site/
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/index.md</summary>

**Chunk 1**
Lines added: 63, lines removed: 1

```diff
@@ -1,1 +1,63 @@
- 
+ # Pense
+ 
+ An open-source simulation and testing framework for voice agents.
+ 
+ With Pense, you can move from a slow, manual testing to a fast, automated, and repeatable testing process:
+ 
+ - **Evaluate different components** of your voice agents across multiple vendors in isolation
+ - **Create comprehensive test suites** that verify both conversational responses and tool usage
+ - **Simulate entire conversations** spanning thousands of scenarios across multiple user personas
+ 
+ Pense is built on top of [pipecat](https://github.com/pipecat-ai/pipecat), a framework for building voice agents.
+ 
+ ## What Pense Does
+ 
+ Pense helps you test and evaluate voice agents by breaking down the pipeline into testable components:
+ 
+ &#96;&#96;&#96;
+ ┌─────────┐     ┌─────────┐     ┌─────────┐
+ │   STT   │ --&gt; │   LLM   │ --&gt; │   TTS   │
+ │(Speech) │     │(Language│     │(Speech) │
+ │  to     │     │ Model)  │     │  to     │
+ │  Text)  │     │         │     │  Text)  │
+ └─────────┘     └─────────┘     └─────────┘
+ &#96;&#96;&#96;
+ 
+ You can evaluate each component independently or test the entire pipeline together.
+ 
+ ## Main Use Cases
+ 
+ ### 1. **Compare STT/TTS Providers**
+ Evaluate different speech-to-text and text-to-speech providers to find the best fit for your use case. Compare accuracy, latency, and cost across multiple vendors.
+ 
+ ### 2. **Test LLM Behavior**
+ Create comprehensive test suites to verify that your LLM responds correctly, calls tools appropriately, and handles edge cases as expected.
+ 
+ ### 3. **Simulate Voice Agent Conversations**
+ Run automated simulations with different user personas and scenarios to test your voice agent at scale before deployment.
+ 
+ ## Quick Start
+ 
+ Ready to get started? Follow our quick start guide:
+ 
+ [Get Started :material-arrow-right:](getting-started/index.md){ .md-button .md-button--primary }
+ 
+ ## Key Features
+ 
+ - **Component Isolation**: Test STT, TTS, and LLM components independently
+ - **Multi-Provider Support**: Evaluate across multiple vendors (Deepgram, OpenAI, Google, ElevenLabs, and more)
+ - **Automated Testing**: Run thousands of test scenarios automatically
+ - **Comprehensive Metrics**: Get detailed metrics including accuracy, latency, and quality scores
+ - **Leaderboards**: Compare providers side-by-side with visual charts and reports
+ 
+ ## Learn More
+ 
+ - [Installation Guide](getting-started/installation.md) - Set up Pense in minutes
+ - [Concepts](concepts/index.md) - Understand how Pense works
+ - [Examples](examples/index.md) - See Pense in action
+ - [Reference](reference/cli-reference.md) - Complete API reference
+ 
+ ---
+ 
+ **Built with ❤️ for the voice agent community**
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/getting-started/index.md</summary>

**Chunk 1**
Lines added: 30, lines removed: 1

```diff
@@ -1,1 +1,30 @@
- 
+ # Getting Started
+ 
+ Welcome to Pense! This guide will help you get up and running quickly.
+ 
+ ## What You'll Learn
+ 
+ In this section, you'll learn how to:
+ 
+ 1. Install Pense and its dependencies
+ 2. Configure API keys for your providers
+ 3. Run your first evaluation
+ 
+ ## Prerequisites
+ 
+ Before you begin, make sure you have:
+ 
+ - **Python 3.10+** installed on your system
+ - **uv** package manager installed ([install uv](https://github.com/astral-sh/uv))
+ - API keys for the providers you want to test (STT, TTS, or LLM)
+ 
+ ## Quick Navigation
+ 
+ - [Installation](installation.md) - Install Pense and dependencies
+ - [Configuration](configuration.md) - Set up API keys and environment
+ - [First Evaluation](first-evaluation.md) - Run your first test
+ 
+ ## Next Steps
+ 
+ Ready to dive in? Start with [Installation](installation.md) to get Pense set up on your system.
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/getting-started/installation.md</summary>

**Chunk 1**
Lines added: 57, lines removed: 1

```diff
@@ -1,1 +1,57 @@
- 
+ # Installation
+ 
+ Install Pense and get ready to start evaluating voice agents.
+ 
+ ## Install uv
+ 
+ Pense uses &#96;uv&#96; to manage dependencies. If you don't have it installed:
+ 
+ &#96;&#96;&#96;bash
+ # macOS and Linux
+ curl -LsSf https://astral.sh/uv/install.sh | sh
+ 
+ # Windows
+ powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
+ &#96;&#96;&#96;
+ 
+ Verify installation:
+ 
+ &#96;&#96;&#96;bash
+ uv --version
+ &#96;&#96;&#96;
+ 
+ ## Install Pense
+ 
+ 1. Clone or navigate to the Pense repository:
+ 
+ &#96;&#96;&#96;bash
+ cd /path/to/agentloop
+ &#96;&#96;&#96;
+ 
+ 2. Create a virtual environment and install dependencies:
+ 
+ &#96;&#96;&#96;bash
+ uv sync --frozen
+ &#96;&#96;&#96;
+ 
+ This will:
+ - Create a virtual environment
+ - Install all required dependencies
+ - Set up the &#96;pense&#96; CLI command
+ 
+ ## Verify Installation
+ 
+ Check that Pense is installed correctly:
+ 
+ &#96;&#96;&#96;bash
+ pense --help
+ &#96;&#96;&#96;
+ 
+ You should see the Pense CLI help menu with available commands.
+ 
+ ## What's Next?
+ 
+ Now that Pense is installed, configure your API keys:
+ 
+ [Configure API Keys :material-arrow-right:](configuration.md){ .md-button }
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/getting-started/configuration.md</summary>

**Chunk 1**
Lines added: 84, lines removed: 1

```diff
@@ -1,1 +1,84 @@
- 
+ # Configuration
+ 
+ Set up API keys and configure Pense to work with your providers.
+ 
+ ## Environment Setup
+ 
+ Pense uses a single &#96;.env&#96; file for all API keys. This file is shared across all modules (STT, TTS, LLM, Agent).
+ 
+ ## Create .env File
+ 
+ 1. Copy the example environment file:
+ 
+ &#96;&#96;&#96;bash
+ cp .env.example .env
+ &#96;&#96;&#96;
+ 
+ 2. Open &#96;.env&#96; in your editor and add your API keys.
+ 
+ ## Required API Keys
+ 
+ Add API keys for the providers you want to use:
+ 
+ ### STT Providers
+ 
+ &#96;&#96;&#96;bash
+ # Deepgram
+ DEEPGRAM_API_KEY=your_deepgram_key
+ 
+ # OpenAI
+ OPENAI_API_KEY=your_openai_key
+ 
+ # Google Cloud
+ GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json
+ 
+ # Sarvam
+ SARVAM_API_KEY=your_sarvam_key
+ &#96;&#96;&#96;
+ 
+ ### TTS Providers
+ 
+ &#96;&#96;&#96;bash
+ # ElevenLabs
+ ELEVENLABS_API_KEY=your_elevenlabs_key
+ 
+ # Cartesia
+ CARTESIA_API_KEY=your_cartesia_key
+ 
+ # OpenAI (same as STT)
+ OPENAI_API_KEY=your_openai_key
+ 
+ # Google Cloud (same as STT)
+ GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json
+ &#96;&#96;&#96;
+ 
+ ### LLM Providers
+ 
+ &#96;&#96;&#96;bash
+ # OpenAI
+ OPENAI_API_KEY=your_openai_key
+ 
+ # OpenRouter
+ OPENROUTER_API_KEY=your_openrouter_key
+ &#96;&#96;&#96;
+ 
+ ## Security Best Practices
+ 
+ !!! warning "Never commit .env files"
+     The &#96;.env&#96; file contains sensitive API keys. Make sure it's in your &#96;.gitignore&#96; file and never commit it to version control.
+ 
+ ## Verify Configuration
+ 
+ Test your configuration by running a simple command:
+ 
+ &#96;&#96;&#96;bash
+ # This will show available providers
+ pense stt eval --help
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ Now that you're configured, run your first evaluation:
+ 
+ [Run Your First Evaluation :material-arrow-right:](first-evaluation.md){ .md-button }
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/getting-started/first-evaluation.md</summary>

**Chunk 1**
Lines added: 78, lines removed: 1

```diff
@@ -1,1 +1,78 @@
- 
+ # Your First Evaluation
+ 
+ Run your first STT evaluation to see Pense in action.
+ 
+ ## Quick Start: STT Evaluation
+ 
+ Let's run a simple STT evaluation using the sample data provided with Pense.
+ 
+ ### Step 1: Prepare Sample Data
+ 
+ Pense includes sample data you can use for testing. It's located at:
+ 
+ &#96;&#96;&#96;
+ pense/stt/examples/sample_input/
+ &#96;&#96;&#96;
+ 
+ ### Step 2: Run the Evaluation
+ 
+ Run an STT evaluation with a provider of your choice:
+ 
+ &#96;&#96;&#96;bash
+ pense stt eval \
+   -p deepgram \
+   -l english \
+   -i pense/stt/examples/sample_input \
+   -o ./out/stt-test \
+   -d
+ &#96;&#96;&#96;
+ 
+ **Parameters explained:**
+ - &#96;-p deepgram&#96;: Provider to use (deepgram, openai, google, sarvam, etc.)
+ - &#96;-l english&#96;: Language (english or hindi)
+ - &#96;-i&#96;: Input directory containing audio files and CSV
+ - &#96;-o&#96;: Output directory for results
+ - &#96;-d&#96;: Debug mode (only processes first 5 files)
+ 
+ ### Step 3: View Results
+ 
+ After the evaluation completes, check the output directory:
+ 
+ &#96;&#96;&#96;bash
+ ls -la ./out/stt-test/
+ &#96;&#96;&#96;
+ 
+ You'll find:
+ - &#96;results.csv&#96; - Detailed results for each audio file
+ - &#96;metrics.json&#96; - Aggregated metrics and latency statistics
+ - &#96;logs/&#96; - Full execution logs
+ 
+ ### Step 4: Understand the Results
+ 
+ Open &#96;results.csv&#96; to see:
+ - **WER** (Word Error Rate): Lower is better
+ - **String Similarity**: Higher is better (0-1 scale)
+ - **LLM Judge Score**: Pass/fail evaluation
+ 
+ Check &#96;metrics.json&#96; for latency metrics:
+ - **TTFB** (Time to First Byte): How quickly the provider responds
+ - **Processing Time**: Total processing time
+ 
+ ## What's Next?
+ 
+ Now that you've run your first evaluation:
+ 
+ - [Learn about STT Evaluation](../guides/stt/index.md) - Deep dive into STT testing
+ - [Explore Concepts](../concepts/index.md) - Understand how Pense works
+ - [See More Examples](../examples/index.md) - Try other evaluation types
+ 
+ ## Troubleshooting
+ 
+ If you encounter issues:
+ 
+ - **API Key Errors**: Check your &#96;.env&#96; file configuration
+ - **File Not Found**: Verify the input directory path
+ - **Provider Errors**: Ensure your API key is valid and has credits
+ 
+ For more help, see the [Troubleshooting](../troubleshooting/index.md) section.
+ 
```

</details>

</tool-use>

---



---


Creating placeholder files for the remaining documentation sections so MkDocs builds successfully:


---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/concepts/index.md</summary>

**Chunk 1**
Lines added: 31, lines removed: 1

```diff
@@ -1,1 +1,31 @@
- 
+ # Core Concepts
+ 
+ Understand the fundamental concepts behind Pense.
+ 
+ ## Overview
+ 
+ Pense is designed around the principle of **component isolation** - testing each part of your voice agent pipeline independently before testing the whole system.
+ 
+ ## Key Concepts
+ 
+ - [Evaluation Workflow](evaluation-workflow.md) - How evaluations work in Pense
+ - [Metrics](metrics.md) - Understanding the metrics Pense calculates
+ - [Providers](providers.md) - Supported providers and their capabilities
+ - [Output Structure](output-structure.md) - Understanding Pense's output files
+ 
+ ## Component Isolation
+ 
+ Voice agents typically consist of three main components:
+ 
+ 1. **STT (Speech-to-Text)**: Converts audio to text
+ 2. **LLM (Large Language Model)**: Processes text and generates responses
+ 3. **TTS (Text-to-Speech)**: Converts text back to audio
+ 
+ Pense allows you to test each component independently, making it easier to identify issues and optimize performance.
+ 
+ ## Next Steps
+ 
+ - Learn about the [evaluation workflow](evaluation-workflow.md)
+ - Understand [metrics](metrics.md) and how to interpret them
+ - Explore [provider options](providers.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/concepts/evaluation-workflow.md</summary>

**Chunk 1**
Lines added: 38, lines removed: 1

```diff
@@ -1,1 +1,38 @@
- 
+ # Evaluation Workflow
+ 
+ How evaluations work in Pense.
+ 
+ ## Overview
+ 
+ Pense follows a simple workflow: **Input → Process → Output**
+ 
+ ## Workflow Steps
+ 
+ 1. **Prepare Input Data**: Organize your test data (audio files, CSV files, or JSON configs)
+ 2. **Run Evaluation**: Execute the evaluation command
+ 3. **Analyze Results**: Review metrics, logs, and output files
+ 
+ ## Component-Specific Workflows
+ 
+ ### STT/TTS Evaluations
+ 
+ - Input: Audio files + ground truth data
+ - Process: Send audio to provider, get transcription/synthesis
+ - Output: Accuracy metrics, latency data, comparison results
+ 
+ ### LLM Evaluations
+ 
+ - Input: Test cases or simulation configs
+ - Process: Run LLM with test inputs, evaluate responses
+ - Output: Pass/fail results, evaluation scores, detailed logs
+ 
+ ### Agent Simulations
+ 
+ - Input: Personas, scenarios, agent configuration
+ - Process: Simulate conversations between agent and personas
+ - Output: Transcripts, tool calls, evaluation results
+ 
+ ## Next Steps
+ 
+ Learn about the [metrics](metrics.md) Pense calculates.
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/concepts/metrics.md</summary>

**Chunk 1**
Lines added: 34, lines removed: 1

```diff
@@ -1,1 +1,34 @@
- 
+ # Metrics
+ 
+ Understanding the metrics Pense calculates.
+ 
+ ## STT Metrics
+ 
+ - **WER (Word Error Rate)**: Percentage of words incorrectly transcribed (lower is better)
+ - **String Similarity**: Similarity score between ground truth and transcription (0-1, higher is better)
+ - **LLM Judge Score**: Pass/fail evaluation by LLM judge
+ - **Latency**: Time to first byte (TTFB) and processing time
+ 
+ ## TTS Metrics
+ 
+ - **Latency**: Time to first byte (TTFB) and processing time
+ - **Audio Quality**: Subjective evaluation (when available)
+ 
+ ## LLM Metrics
+ 
+ - **Pass Rate**: Percentage of tests that passed
+ - **Tool Call Accuracy**: Correctness of tool calls
+ - **Response Quality**: LLM judge evaluation scores
+ 
+ ## Understanding Metrics
+ 
+ Metrics help you:
+ - Compare providers objectively
+ - Identify performance bottlenecks
+ - Make informed decisions about which providers to use
+ 
+ ## Next Steps
+ 
+ - Learn about [providers](providers.md) and their capabilities
+ - Understand [output structure](output-structure.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/concepts/providers.md</summary>

**Chunk 1**
Lines added: 41, lines removed: 1

```diff
@@ -1,1 +1,41 @@
- 
+ # Providers
+ 
+ Supported providers and their capabilities.
+ 
+ ## STT Providers
+ 
+ - **Deepgram**: High accuracy, low latency
+ - **OpenAI**: Whisper-based, good accuracy
+ - **Google**: Cloud Speech-to-Text, multilingual support
+ - **Sarvam**: Indian language support
+ - **Cartesia**: Fast, low latency
+ - **Smallest**: Lightweight option
+ 
+ ## TTS Providers
+ 
+ - **ElevenLabs**: High-quality voices, natural speech
+ - **Cartesia**: Fast, low latency
+ - **OpenAI**: TTS API, good quality
+ - **Google**: Cloud Text-to-Speech, multilingual
+ - **Deepgram**: Fast synthesis
+ - **Sarvam**: Indian language support
+ - **Smallest**: Lightweight option
+ 
+ ## LLM Providers
+ 
+ - **OpenAI**: GPT models (GPT-4, GPT-3.5)
+ - **OpenRouter**: Access to multiple models
+ 
+ ## Choosing a Provider
+ 
+ Consider:
+ - **Accuracy**: How accurate are the results?
+ - **Latency**: How fast is the response?
+ - **Cost**: What's the pricing?
+ - **Language Support**: Does it support your target languages?
+ 
+ ## Next Steps
+ 
+ - See detailed [provider reference](../reference/providers.md)
+ - Learn about [output structure](output-structure.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/concepts/output-structure.md</summary>

**Chunk 1**
Lines added: 53, lines removed: 1

```diff
@@ -1,1 +1,53 @@
- 
+ # Output Structure
+ 
+ Understanding Pense's output files.
+ 
+ ## Common Output Structure
+ 
+ All evaluations create output directories with:
+ 
+ &#96;&#96;&#96;
+ output_dir/
+ ├── results.csv          # Detailed results
+ ├── metrics.json         # Aggregated metrics
+ ├── logs/                # Execution logs
+ └── [component-specific files]
+ &#96;&#96;&#96;
+ 
+ ## File Types
+ 
+ ### CSV Files
+ 
+ - **results.csv**: Row per test case with metrics
+ - **leaderboard.csv**: Comparison across providers/runs
+ 
+ ### JSON Files
+ 
+ - **metrics.json**: Aggregated statistics
+ - **transcripts.json**: Conversation transcripts (agent simulations)
+ - **tool_calls.json**: Tool call history (agent simulations)
+ 
+ ### Logs
+ 
+ - **logs/**: Full execution logs for debugging
+ - **results.log**: Terminal output summary
+ 
+ ## Component-Specific Outputs
+ 
+ ### STT/TTS
+ 
+ - Audio files (for TTS)
+ - Transcription results
+ 
+ ### Agent Simulations
+ 
+ - &#96;audios/&#96;: Audio files for each turn
+ - &#96;transcripts.json&#96;: Full conversation
+ - &#96;tool_calls.json&#96;: Tool invocations
+ - &#96;stt_outputs.json&#96;: STT results per turn
+ 
+ ## Next Steps
+ 
+ - Explore [guides](../guides/stt/index.md) for component-specific details
+ - Check [examples](../examples/index.md) to see outputs in action
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/stt/index.md</summary>

**Chunk 1**
Lines added: 26, lines removed: 1

```diff
@@ -1,1 +1,26 @@
- 
+ # STT Evaluation
+ 
+ Evaluate speech-to-text providers with Pense.
+ 
+ ## Overview
+ 
+ STT evaluation helps you compare different speech-to-text providers on accuracy, latency, and cost.
+ 
+ ## Quick Links
+ 
+ - [Preparing Data](preparing-data.md) - Set up your test data
+ - [Running Evaluations](running-eval.md) - Execute STT evaluations
+ - [Understanding Results](understanding-results.md) - Interpret output
+ - [Leaderboards](leaderboard.md) - Compare providers
+ 
+ ## What You'll Learn
+ 
+ - How to prepare audio files and ground truth data
+ - How to run evaluations across multiple providers
+ - How to interpret accuracy and latency metrics
+ - How to generate comparison leaderboards
+ 
+ ## Next Steps
+ 
+ Start by [preparing your data](preparing-data.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/stt/preparing-data.md</summary>

**Chunk 1**
Lines added: 45, lines removed: 1

```diff
@@ -1,1 +1,45 @@
- 
+ # Preparing STT Data
+ 
+ Set up your test data for STT evaluation.
+ 
+ ## Directory Structure
+ 
+ Organize your data as follows:
+ 
+ &#96;&#96;&#96;
+ data/
+ ├── stt.csv
+ ├── audios/
+ │   ├── wav/
+ │   │   ├── audio_1.wav
+ │   │   └── audio_2.wav
+ │   └── pcm16/
+ │       ├── audio_1.wav
+ │       └── audio_2.wav
+ &#96;&#96;&#96;
+ 
+ ## CSV Format
+ 
+ &#96;stt.csv&#96; should have two columns:
+ 
+ &#96;&#96;&#96;csv
+ id,text
+ audio_1,"Hi"
+ audio_2,"Madam, my name is Geeta Shankar"
+ &#96;&#96;&#96;
+ 
+ - **id**: Matches the audio filename (without extension)
+ - **text**: Ground truth transcription
+ 
+ ## Audio Formats
+ 
+ Provide audio in both formats:
+ - **wav**: Standard WAV format
+ - **pcm16**: PCM16 format (some providers require this)
+ 
+ Pense will automatically select the correct format for each provider.
+ 
+ ## Next Steps
+ 
+ [Run your evaluation](running-eval.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/stt/running-eval.md</summary>

**Chunk 1**
Lines added: 37, lines removed: 1

```diff
@@ -1,1 +1,37 @@
- 
+ # Running STT Evaluations
+ 
+ Execute STT evaluations with Pense.
+ 
+ ## Basic Command
+ 
+ &#96;&#96;&#96;bash
+ pense stt eval \
+   -p &lt;provider&gt; \
+   -l &lt;language&gt; \
+   -i &lt;input-dir&gt; \
+   -o &lt;output-dir&gt;
+ &#96;&#96;&#96;
+ 
+ ## Parameters
+ 
+ - **&#96;-p, --provider&#96;**: Provider name (deepgram, openai, google, sarvam, etc.)
+ - **&#96;-l, --language&#96;**: Language (english, hindi)
+ - **&#96;-i, --input-dir&#96;**: Path to input directory
+ - **&#96;-o, --output-dir&#96;**: Path to output directory
+ - **&#96;-d, --debug&#96;**: Debug mode (process first 5 files only)
+ 
+ ## Example
+ 
+ &#96;&#96;&#96;bash
+ pense stt eval \
+   -p deepgram \
+   -l english \
+   -i ./data/stt-test \
+   -o ./out/deepgram-results \
+   -d
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ [Understand your results](understanding-results.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/stt/understanding-results.md</summary>

**Chunk 1**
Lines added: 42, lines removed: 1

```diff
@@ -1,1 +1,42 @@
- 
+ # Understanding STT Results
+ 
+ Interpret STT evaluation outputs.
+ 
+ ## Output Files
+ 
+ ### results.csv
+ 
+ Contains per-file results:
+ 
+ &#96;&#96;&#96;csv
+ id,gt,pred,wer,string_similarity,llm_judge_score,llm_judge_reasoning
+ audio_1,"Hi","Hi",0.0,1.0,True,"Perfect match"
+ &#96;&#96;&#96;
+ 
+ - **wer**: Word Error Rate (lower is better)
+ - **string_similarity**: Similarity score 0-1 (higher is better)
+ - **llm_judge_score**: Pass/fail evaluation
+ 
+ ### metrics.json
+ 
+ Aggregated statistics:
+ 
+ &#96;&#96;&#96;json
+ {
+   "wer": 0.13,
+   "string_similarity": 0.88,
+   "llm_judge_score": 1.0,
+   "metrics": [...]
+ }
+ &#96;&#96;&#96;
+ 
+ ## Interpreting Metrics
+ 
+ - **WER &lt; 0.1**: Excellent accuracy
+ - **WER 0.1-0.2**: Good accuracy
+ - **WER &gt; 0.2**: May need improvement
+ 
+ ## Next Steps
+ 
+ [Generate leaderboards](leaderboard.md) to compare providers.
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/stt/leaderboard.md</summary>

**Chunk 1**
Lines added: 25, lines removed: 1

```diff
@@ -1,1 +1,25 @@
- 
+ # STT Leaderboards
+ 
+ Compare STT providers side-by-side.
+ 
+ ## Generate Leaderboard
+ 
+ After running multiple provider evaluations:
+ 
+ &#96;&#96;&#96;bash
+ pense stt leaderboard \
+   -o ./out \
+   -s ./leaderboards
+ &#96;&#96;&#96;
+ 
+ ## Output
+ 
+ Creates:
+ - **stt_leaderboard.xlsx**: Excel file with all metrics
+ - **all_metrics_by_run.png**: Visual comparison chart
+ 
+ ## Next Steps
+ 
+ - Try [TTS evaluation](../tts/index.md)
+ - Explore [LLM testing](../llm/index.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/tts/index.md</summary>

**Chunk 1**
Lines added: 19, lines removed: 1

```diff
@@ -1,1 +1,19 @@
- 
+ # TTS Evaluation
+ 
+ Evaluate text-to-speech providers with Pense.
+ 
+ ## Overview
+ 
+ TTS evaluation helps you compare different text-to-speech providers on quality, latency, and cost.
+ 
+ ## Quick Links
+ 
+ - [Preparing Data](preparing-data.md)
+ - [Running Evaluations](running-eval.md)
+ - [Understanding Results](understanding-results.md)
+ - [Leaderboards](leaderboard.md)
+ 
+ ## Next Steps
+ 
+ Start by [preparing your data](preparing-data.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/tts/preparing-data.md</summary>

**Chunk 1**
Lines added: 28, lines removed: 1

```diff
@@ -1,1 +1,28 @@
- 
+ # Preparing TTS Data
+ 
+ Set up your test data for TTS evaluation.
+ 
+ ## CSV Format
+ 
+ Create a CSV file with a single &#96;text&#96; column:
+ 
+ &#96;&#96;&#96;csv
+ text
+ hello world
+ this is a test
+ &#96;&#96;&#96;
+ 
+ ## Example File
+ 
+ Save as &#96;tts-input.csv&#96;:
+ 
+ &#96;&#96;&#96;csv
+ text
+ Hello, how are you today?
+ The weather is beautiful today.
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ [Run your evaluation](running-eval.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/tts/running-eval.md</summary>

**Chunk 1**
Lines added: 28, lines removed: 1

```diff
@@ -1,1 +1,28 @@
- 
+ # Running TTS Evaluations
+ 
+ Execute TTS evaluations with Pense.
+ 
+ ## Basic Command
+ 
+ &#96;&#96;&#96;bash
+ pense tts eval \
+   -p &lt;provider&gt; \
+   -l &lt;language&gt; \
+   -i &lt;input.csv&gt; \
+   -o &lt;output-dir&gt;
+ &#96;&#96;&#96;
+ 
+ ## Example
+ 
+ &#96;&#96;&#96;bash
+ pense tts eval \
+   -p elevenlabs \
+   -l english \
+   -i ./data/tts-input.csv \
+   -o ./out/elevenlabs-results
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ [Understand your results](understanding-results.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/tts/understanding-results.md</summary>

**Chunk 1**
Lines added: 31, lines removed: 1

```diff
@@ -1,1 +1,31 @@
- 
+ # Understanding TTS Results
+ 
+ Interpret TTS evaluation outputs.
+ 
+ ## Output Files
+ 
+ ### results.csv
+ 
+ &#96;&#96;&#96;csv
+ text,audio_path
+ hello world,./out/audios/1.wav
+ &#96;&#96;&#96;
+ 
+ ### metrics.json
+ 
+ Latency metrics:
+ 
+ &#96;&#96;&#96;json
+ [
+   {
+     "metric_name": "ttfb",
+     "mean": 0.35,
+     "std": 0.03
+   }
+ ]
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ [Generate leaderboards](leaderboard.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/tts/leaderboard.md</summary>

**Chunk 1**
Lines added: 17, lines removed: 1

```diff
@@ -1,1 +1,17 @@
- 
+ # TTS Leaderboards
+ 
+ Compare TTS providers side-by-side.
+ 
+ ## Generate Leaderboard
+ 
+ &#96;&#96;&#96;bash
+ pense tts leaderboard \
+   -o ./out \
+   -s ./leaderboards
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ - Explore [LLM testing](../llm/index.md)
+ - Try [agent simulations](../agent/index.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/llm/index.md</summary>

**Chunk 1**
Lines added: 22, lines removed: 1

```diff
@@ -1,1 +1,22 @@
- 
+ # LLM Evaluation
+ 
+ Evaluate LLM behavior with Pense.
+ 
+ ## Overview
+ 
+ Pense supports two types of LLM evaluation:
+ 
+ 1. **Tests**: Structured test cases with expected outputs
+ 2. **Simulations**: Automated conversations with personas
+ 
+ ## Quick Links
+ 
+ - [LLM Tests](tests/index.md) - Structured test cases
+ - [LLM Simulations](simulations/index.md) - Automated conversations
+ 
+ ## Next Steps
+ 
+ Choose your evaluation type:
+ - [LLM Tests](tests/index.md) - For verifying specific behaviors
+ - [LLM Simulations](simulations/index.md) - For end-to-end testing
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/llm/tests/index.md</summary>

**Chunk 1**
Lines added: 22, lines removed: 1

```diff
@@ -1,1 +1,22 @@
- 
+ # LLM Tests
+ 
+ Create structured test cases for LLM evaluation.
+ 
+ ## Overview
+ 
+ LLM tests verify specific behaviors:
+ - Tool calls are made correctly
+ - Responses meet quality criteria
+ - Edge cases are handled properly
+ 
+ ## Quick Links
+ 
+ - [Creating Tests](creating-tests.md)
+ - [Running Tests](running-tests.md)
+ - [Evaluation Types](evaluation-types.md)
+ - [Leaderboards](leaderboard.md)
+ 
+ ## Next Steps
+ 
+ [Create your first test](creating-tests.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/llm/tests/creating-tests.md</summary>

**Chunk 1**
Lines added: 31, lines removed: 1

```diff
@@ -1,1 +1,31 @@
- 
+ # Creating LLM Tests
+ 
+ Set up test configurations for LLM evaluation.
+ 
+ ## Test Configuration Structure
+ 
+ &#96;&#96;&#96;json
+ {
+   "system_prompt": "You are a helpful assistant.",
+   "tools": [...],
+   "test_cases": [
+     {
+       "history": [...],
+       "evaluation": {
+         "type": "tool_call",
+         "tool_calls": [...]
+       }
+     }
+   ]
+ }
+ &#96;&#96;&#96;
+ 
+ ## Evaluation Types
+ 
+ - **tool_call**: Verify specific tool calls
+ - **response**: Evaluate response quality with LLM judge
+ 
+ ## Next Steps
+ 
+ [Run your tests](running-tests.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/llm/tests/running-tests.md</summary>

**Chunk 1**
Lines added: 26, lines removed: 1

```diff
@@ -1,1 +1,26 @@
- 
+ # Running LLM Tests
+ 
+ Execute LLM test suites.
+ 
+ ## Basic Command
+ 
+ &#96;&#96;&#96;bash
+ pense llm tests run \
+   -c &lt;config.json&gt; \
+   -o &lt;output-dir&gt; \
+   -m &lt;model&gt;
+ &#96;&#96;&#96;
+ 
+ ## Example
+ 
+ &#96;&#96;&#96;bash
+ pense llm tests run \
+   -c ./configs/test-suite.json \
+   -o ./out/llm-tests \
+   -m gpt-4.1
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ [Understand evaluation types](evaluation-types.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/llm/tests/evaluation-types.md</summary>

**Chunk 1**
Lines added: 35, lines removed: 1

```diff
@@ -1,1 +1,35 @@
- 
+ # LLM Test Evaluation Types
+ 
+ Two ways to evaluate LLM behavior.
+ 
+ ## Tool Call Evaluation
+ 
+ Verifies that the LLM calls tools correctly:
+ 
+ &#96;&#96;&#96;json
+ {
+   "type": "tool_call",
+   "tool_calls": [
+     {
+       "tool": "get_weather",
+       "arguments": {"location": "San Francisco"}
+     }
+   ]
+ }
+ &#96;&#96;&#96;
+ 
+ ## Response Evaluation
+ 
+ Uses an LLM judge to evaluate response quality:
+ 
+ &#96;&#96;&#96;json
+ {
+   "type": "response",
+   "criteria": "The assistant should be helpful and concise"
+ }
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ [Generate leaderboards](leaderboard.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/llm/tests/leaderboard.md</summary>

**Chunk 1**
Lines added: 17, lines removed: 1

```diff
@@ -1,1 +1,17 @@
- 
+ # LLM Test Leaderboards
+ 
+ Compare test results across models.
+ 
+ ## Generate Leaderboard
+ 
+ &#96;&#96;&#96;bash
+ pense llm tests leaderboard \
+   -o ./out \
+   -s ./leaderboards
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ - Try [LLM simulations](../simulations/index.md)
+ - Explore [agent testing](../../agent/index.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/llm/simulations/index.md</summary>

**Chunk 1**
Lines added: 22, lines removed: 1

```diff
@@ -1,1 +1,22 @@
- 
+ # LLM Simulations
+ 
+ Run automated conversations to test LLM behavior.
+ 
+ ## Overview
+ 
+ LLM simulations create conversations between:
+ - Your agent (LLM with tools)
+ - User personas (simulated users)
+ 
+ ## Quick Links
+ 
+ - [Creating Config](creating-config.md)
+ - [Personas &amp; Scenarios](personas-scenarios.md)
+ - [Evaluation Criteria](evaluation-criteria.md)
+ - [Running Simulations](running-simulations.md)
+ - [Leaderboards](leaderboard.md)
+ 
+ ## Next Steps
+ 
+ [Create your simulation config](creating-config.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/llm/simulations/creating-config.md</summary>

**Chunk 1**
Lines added: 21, lines removed: 1

```diff
@@ -1,1 +1,21 @@
- 
+ # Creating LLM Simulation Configs
+ 
+ Set up simulation configurations.
+ 
+ ## Configuration Structure
+ 
+ &#96;&#96;&#96;json
+ {
+   "agent_system_prompt": "...",
+   "tools": [...],
+   "personas": [...],
+   "scenarios": [...],
+   "evaluation_criteria": [...]
+ }
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ - Learn about [personas and scenarios](personas-scenarios.md)
+ - Set up [evaluation criteria](evaluation-criteria.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/llm/simulations/personas-scenarios.md</summary>

**Chunk 1**
Lines added: 24, lines removed: 1

```diff
@@ -1,1 +1,24 @@
- 
+ # Personas and Scenarios
+ 
+ Define user personas and test scenarios.
+ 
+ ## Personas
+ 
+ Personas describe simulated users:
+ 
+ &#96;&#96;&#96;
+ "You are a mother; name is Geeta Prasad, 39 years old..."
+ &#96;&#96;&#96;
+ 
+ ## Scenarios
+ 
+ Scenarios define test situations:
+ 
+ &#96;&#96;&#96;
+ "the mother answers all questions and completes the form"
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ [Set up evaluation criteria](evaluation-criteria.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/llm/simulations/evaluation-criteria.md</summary>

**Chunk 1**
Lines added: 17, lines removed: 1

```diff
@@ -1,1 +1,17 @@
- 
+ # Evaluation Criteria
+ 
+ Define how to evaluate simulations.
+ 
+ ## Criteria Structure
+ 
+ &#96;&#96;&#96;json
+ {
+   "name": "question_completeness",
+   "description": "Whether all questions were covered"
+ }
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ [Run your simulations](running-simulations.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/llm/simulations/running-simulations.md</summary>

**Chunk 1**
Lines added: 26, lines removed: 1

```diff
@@ -1,1 +1,26 @@
- 
+ # Running LLM Simulations
+ 
+ Execute LLM simulations.
+ 
+ ## Basic Command
+ 
+ &#96;&#96;&#96;bash
+ pense llm simulations run \
+   -c &lt;config.json&gt; \
+   -o &lt;output-dir&gt; \
+   -m &lt;model&gt;
+ &#96;&#96;&#96;
+ 
+ ## Example
+ 
+ &#96;&#96;&#96;bash
+ pense llm simulations run \
+   -c ./configs/simulation.json \
+   -o ./out/simulations \
+   -m gpt-4.1
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ [Generate leaderboards](leaderboard.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/llm/simulations/leaderboard.md</summary>

**Chunk 1**
Lines added: 17, lines removed: 1

```diff
@@ -1,1 +1,17 @@
- 
+ # LLM Simulation Leaderboards
+ 
+ Compare simulation results.
+ 
+ ## Generate Leaderboard
+ 
+ &#96;&#96;&#96;bash
+ pense llm simulations leaderboard \
+   -o ./out \
+   -s ./leaderboards
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ - Try [agent simulations](../agent/index.md)
+ - Explore [examples](../../examples/index.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/agent/index.md</summary>

**Chunk 1**
Lines added: 22, lines removed: 1

```diff
@@ -1,1 +1,22 @@
- 
+ # Agent Evaluation
+ 
+ Test complete voice agents with STT, LLM, and TTS.
+ 
+ ## Overview
+ 
+ Agent evaluation tests the full pipeline:
+ - Interactive testing with real audio
+ - Automated simulations with personas
+ 
+ ## Quick Links
+ 
+ - [Interactive Testing](interactive-testing.md)
+ - [Agent Simulations](agent-simulations.md)
+ - [Config Reference](config-reference.md)
+ - [Understanding Outputs](understanding-outputs.md)
+ 
+ ## Next Steps
+ 
+ - [Try interactive testing](interactive-testing.md)
+ - [Run agent simulations](agent-simulations.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/agent/interactive-testing.md</summary>

**Chunk 1**
Lines added: 25, lines removed: 1

```diff
@@ -1,1 +1,25 @@
- 
+ # Interactive Agent Testing
+ 
+ Test your agent with real audio input.
+ 
+ ## Overview
+ 
+ Interactive testing lets you speak with your agent through a web interface.
+ 
+ ## Running Interactive Test
+ 
+ &#96;&#96;&#96;bash
+ pense agent test \
+   -c &lt;config.json&gt; \
+   -o &lt;output-dir&gt;
+ &#96;&#96;&#96;
+ 
+ ## Web Interface
+ 
+ Open &#96;http://localhost:7860/client/&#96; in your browser.
+ 
+ ## Next Steps
+ 
+ - [Run agent simulations](agent-simulations.md)
+ - [Understand outputs](understanding-outputs.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/agent/agent-simulations.md</summary>

**Chunk 1**
Lines added: 21, lines removed: 1

```diff
@@ -1,1 +1,21 @@
- 
+ # Agent Simulations
+ 
+ Run automated voice agent simulations.
+ 
+ ## Overview
+ 
+ Agent simulations test the full STT → LLM → TTS pipeline with personas.
+ 
+ ## Running Simulations
+ 
+ &#96;&#96;&#96;bash
+ pense agent simulation \
+   -c &lt;config.json&gt; \
+   -o &lt;output-dir&gt;
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ - [Config reference](config-reference.md)
+ - [Understanding outputs](understanding-outputs.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/agent/config-reference.md</summary>

**Chunk 1**
Lines added: 20, lines removed: 1

```diff
@@ -1,1 +1,20 @@
- 
+ # Agent Configuration Reference
+ 
+ Complete reference for agent configuration files.
+ 
+ ## Configuration Structure
+ 
+ &#96;&#96;&#96;json
+ {
+   "agent_system_prompt": "...",
+   "language": "english",
+   "tools": [...],
+   "personas": [...],
+   "scenarios": [...]
+ }
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ [Understand simulation outputs](understanding-outputs.md).
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/guides/agent/understanding-outputs.md</summary>

**Chunk 1**
Lines added: 28, lines removed: 1

```diff
@@ -1,1 +1,28 @@
- 
+ # Understanding Agent Outputs
+ 
+ Interpret agent simulation results.
+ 
+ ## Output Structure
+ 
+ &#96;&#96;&#96;
+ output_dir/
+ ├── simulation_persona_1_scenario_1/
+ │   ├── audios/
+ │   ├── transcripts.json
+ │   ├── tool_calls.json
+ │   ├── metrics.json
+ │   └── logs/
+ &#96;&#96;&#96;
+ 
+ ## Key Files
+ 
+ - **transcripts.json**: Full conversation
+ - **tool_calls.json**: Tool invocations
+ - **metrics.json**: Latency and performance metrics
+ - **audios/**: Audio files for each turn
+ 
+ ## Next Steps
+ 
+ - Explore [examples](../../examples/index.md)
+ - Check [reference docs](../../reference/cli-reference.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/reference/cli-reference.md</summary>

**Chunk 1**
Lines added: 60, lines removed: 1

```diff
@@ -1,1 +1,60 @@
- 
+ # CLI Reference
+ 
+ Complete command-line reference for Pense.
+ 
+ ## STT Commands
+ 
+ ### Evaluate STT
+ 
+ &#96;&#96;&#96;bash
+ pense stt eval -p &lt;provider&gt; -l &lt;language&gt; -i &lt;input-dir&gt; -o &lt;output-dir&gt;
+ &#96;&#96;&#96;
+ 
+ ### Generate Leaderboard
+ 
+ &#96;&#96;&#96;bash
+ pense stt leaderboard -o &lt;output-dir&gt; -s &lt;save-dir&gt;
+ &#96;&#96;&#96;
+ 
+ ## TTS Commands
+ 
+ ### Evaluate TTS
+ 
+ &#96;&#96;&#96;bash
+ pense tts eval -p &lt;provider&gt; -l &lt;language&gt; -i &lt;input.csv&gt; -o &lt;output-dir&gt;
+ &#96;&#96;&#96;
+ 
+ ### Generate Leaderboard
+ 
+ &#96;&#96;&#96;bash
+ pense tts leaderboard -o &lt;output-dir&gt; -s &lt;save-dir&gt;
+ &#96;&#96;&#96;
+ 
+ ## LLM Commands
+ 
+ ### Run Tests
+ 
+ &#96;&#96;&#96;bash
+ pense llm tests run -c &lt;config.json&gt; -o &lt;output-dir&gt; -m &lt;model&gt;
+ &#96;&#96;&#96;
+ 
+ ### Run Simulations
+ 
+ &#96;&#96;&#96;bash
+ pense llm simulations run -c &lt;config.json&gt; -o &lt;output-dir&gt; -m &lt;model&gt;
+ &#96;&#96;&#96;
+ 
+ ## Agent Commands
+ 
+ ### Interactive Test
+ 
+ &#96;&#96;&#96;bash
+ pense agent test -c &lt;config.json&gt; -o &lt;output-dir&gt;
+ &#96;&#96;&#96;
+ 
+ ### Simulation
+ 
+ &#96;&#96;&#96;bash
+ pense agent simulation -c &lt;config.json&gt; -o &lt;output-dir&gt;
+ &#96;&#96;&#96;
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/reference/config-reference.md</summary>

**Chunk 1**
Lines added: 41, lines removed: 1

```diff
@@ -1,1 +1,41 @@
- 
+ # Configuration Reference
+ 
+ Reference for all configuration file formats.
+ 
+ ## STT/TTS Config
+ 
+ No separate config file - uses command-line arguments.
+ 
+ ## LLM Test Config
+ 
+ &#96;&#96;&#96;json
+ {
+   "system_prompt": "...",
+   "tools": [...],
+   "test_cases": [...]
+ }
+ &#96;&#96;&#96;
+ 
+ ## LLM Simulation Config
+ 
+ &#96;&#96;&#96;json
+ {
+   "agent_system_prompt": "...",
+   "personas": [...],
+   "scenarios": [...],
+   "evaluation_criteria": [...]
+ }
+ &#96;&#96;&#96;
+ 
+ ## Agent Config
+ 
+ &#96;&#96;&#96;json
+ {
+   "agent_system_prompt": "...",
+   "language": "english",
+   "tools": [...],
+   "personas": [...],
+   "scenarios": [...]
+ }
+ &#96;&#96;&#96;
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/reference/providers.md</summary>

**Chunk 1**
Lines added: 28, lines removed: 1

```diff
@@ -1,1 +1,28 @@
- 
+ # Providers Reference
+ 
+ Detailed information about supported providers.
+ 
+ ## STT Providers
+ 
+ - Deepgram
+ - OpenAI
+ - Google Cloud
+ - Sarvam
+ - Cartesia
+ - Smallest
+ 
+ ## TTS Providers
+ 
+ - ElevenLabs
+ - Cartesia
+ - OpenAI
+ - Google Cloud
+ - Deepgram
+ - Sarvam
+ - Smallest
+ 
+ ## LLM Providers
+ 
+ - OpenAI
+ - OpenRouter
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/reference/metrics-reference.md</summary>

**Chunk 1**
Lines added: 27, lines removed: 1

```diff
@@ -1,1 +1,27 @@
- 
+ # Metrics Reference
+ 
+ Detailed documentation of all metrics.
+ 
+ ## STT Metrics
+ 
+ - WER (Word Error Rate)
+ - String Similarity
+ - LLM Judge Score
+ - Latency (TTFB, Processing Time)
+ 
+ ## TTS Metrics
+ 
+ - Latency (TTFB, Processing Time)
+ 
+ ## LLM Metrics
+ 
+ - Pass Rate
+ - Tool Call Accuracy
+ - Response Quality Score
+ 
+ ## Agent Metrics
+ 
+ - End-to-end Latency
+ - Component-specific Latency
+ - Tool Call Metrics
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/examples/index.md</summary>

**Chunk 1**
Lines added: 16, lines removed: 1

```diff
@@ -1,1 +1,16 @@
- 
+ # Examples
+ 
+ See Pense in action with practical examples.
+ 
+ ## Quick Start Examples
+ 
+ - [STT Quick Evaluation](stt-quick-eval.md)
+ - [TTS Quick Evaluation](tts-quick-eval.md)
+ - [LLM Test Example](llm-test-example.md)
+ - [LLM Simulation Example](llm-simulation-example.md)
+ - [Agent Simulation Example](agent-simulation-example.md)
+ 
+ ## Next Steps
+ 
+ Choose an example to get started!
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/examples/stt-quick-eval.md</summary>

**Chunk 1**
Lines added: 20, lines removed: 1

```diff
@@ -1,1 +1,20 @@
- 
+ # STT Quick Evaluation Example
+ 
+ Run your first STT evaluation.
+ 
+ ## Example
+ 
+ &#96;&#96;&#96;bash
+ pense stt eval \
+   -p deepgram \
+   -l english \
+   -i pense/stt/examples/sample_input \
+   -o ./out/stt-test \
+   -d
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ - [TTS Example](tts-quick-eval.md)
+ - [LLM Test Example](llm-test-example.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/examples/tts-quick-eval.md</summary>

**Chunk 1**
Lines added: 19, lines removed: 1

```diff
@@ -1,1 +1,19 @@
- 
+ # TTS Quick Evaluation Example
+ 
+ Run your first TTS evaluation.
+ 
+ ## Example
+ 
+ &#96;&#96;&#96;bash
+ pense tts eval \
+   -p elevenlabs \
+   -l english \
+   -i pense/tts/examples/sample.csv \
+   -o ./out/tts-test
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ - [LLM Test Example](llm-test-example.md)
+ - [LLM Simulation Example](llm-simulation-example.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/examples/llm-test-example.md</summary>

**Chunk 1**
Lines added: 18, lines removed: 1

```diff
@@ -1,1 +1,18 @@
- 
+ # LLM Test Example
+ 
+ Run your first LLM test.
+ 
+ ## Example
+ 
+ &#96;&#96;&#96;bash
+ pense llm tests run \
+   -c pense/llm/examples/tests/config.json \
+   -o ./out/llm-tests \
+   -m gpt-4.1
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ - [LLM Simulation Example](llm-simulation-example.md)
+ - [Agent Simulation Example](agent-simulation-example.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/examples/llm-simulation-example.md</summary>

**Chunk 1**
Lines added: 17, lines removed: 1

```diff
@@ -1,1 +1,17 @@
- 
+ # LLM Simulation Example
+ 
+ Run your first LLM simulation.
+ 
+ ## Example
+ 
+ &#96;&#96;&#96;bash
+ pense llm simulations run \
+   -c pense/llm/examples/simulation/config.json \
+   -o ./out/simulations \
+   -m gpt-4.1
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ - [Agent Simulation Example](agent-simulation-example.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/examples/agent-simulation-example.md</summary>

**Chunk 1**
Lines added: 17, lines removed: 1

```diff
@@ -1,1 +1,17 @@
- 
+ # Agent Simulation Example
+ 
+ Run your first agent simulation.
+ 
+ ## Example
+ 
+ &#96;&#96;&#96;bash
+ pense agent simulation \
+   -c pense/agent/examples/sample_short_english.json \
+   -o ./out/agent-sim
+ &#96;&#96;&#96;
+ 
+ ## Next Steps
+ 
+ - Explore [guides](../guides/stt/index.md)
+ - Check [reference docs](../reference/cli-reference.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/troubleshooting/index.md</summary>

**Chunk 1**
Lines added: 14, lines removed: 1

```diff
@@ -1,1 +1,14 @@
- 
+ # Troubleshooting
+ 
+ Common issues and solutions.
+ 
+ ## Common Issues
+ 
+ - [API Keys](api-keys.md) - API key setup problems
+ - [Data Format](data-format.md) - Data format errors
+ - [Performance](performance.md) - Performance issues
+ 
+ ## Getting Help
+ 
+ If you can't find a solution here, check the [FAQ](../faq.md) or open an issue on GitHub.
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/troubleshooting/api-keys.md</summary>

**Chunk 1**
Lines added: 23, lines removed: 1

```diff
@@ -1,1 +1,23 @@
- 
+ # API Key Issues
+ 
+ Troubleshooting API key problems.
+ 
+ ## Common Problems
+ 
+ ### API Key Not Found
+ 
+ Make sure your &#96;.env&#96; file exists and contains the required keys.
+ 
+ ### Invalid API Key
+ 
+ Verify your API key is correct and has not expired.
+ 
+ ### Provider-Specific Issues
+ 
+ Check provider documentation for specific requirements.
+ 
+ ## Next Steps
+ 
+ - [Data Format Issues](data-format.md)
+ - [Performance Issues](performance.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/troubleshooting/data-format.md</summary>

**Chunk 1**
Lines added: 20, lines removed: 1

```diff
@@ -1,1 +1,20 @@
- 
+ # Data Format Issues
+ 
+ Troubleshooting data format problems.
+ 
+ ## Common Problems
+ 
+ ### CSV Format Errors
+ 
+ Ensure CSV files follow the required format:
+ - STT: &#96;id,text&#96; columns
+ - TTS: &#96;text&#96; column
+ 
+ ### Audio Format Issues
+ 
+ Provide audio in both WAV and PCM16 formats for STT.
+ 
+ ## Next Steps
+ 
+ - [Performance Issues](performance.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/troubleshooting/performance.md</summary>

**Chunk 1**
Lines added: 14, lines removed: 1

```diff
@@ -1,1 +1,14 @@
- 
+ # Performance Issues
+ 
+ Optimizing evaluation performance.
+ 
+ ## Tips
+ 
+ - Use debug mode (&#96;-d&#96;) for quick tests
+ - Process data in batches
+ - Monitor API rate limits
+ 
+ ## Next Steps
+ 
+ - Check [FAQ](../faq.md)
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/docs/faq.md</summary>

**Chunk 1**
Lines added: 43, lines removed: 1

```diff
@@ -1,1 +1,43 @@
- 
+ # Frequently Asked Questions
+ 
+ Common questions about Pense.
+ 
+ ## General
+ 
+ ### What is Pense?
+ 
+ Pense is an open-source simulation and testing framework for voice agents.
+ 
+ ### What can I test with Pense?
+ 
+ - STT (Speech-to-Text) providers
+ - TTS (Text-to-Speech) providers
+ - LLM behavior and responses
+ - Complete voice agent pipelines
+ 
+ ## Getting Started
+ 
+ ### How do I install Pense?
+ 
+ See the [Installation Guide](getting-started/installation.md).
+ 
+ ### How do I set up API keys?
+ 
+ See the [Configuration Guide](getting-started/configuration.md).
+ 
+ ## Usage
+ 
+ ### Which provider should I use?
+ 
+ It depends on your needs. Compare providers using leaderboards to find the best fit.
+ 
+ ### How do I interpret metrics?
+ 
+ See the [Metrics Guide](concepts/metrics.md).
+ 
+ ## Next Steps
+ 
+ - [Getting Started](getting-started/index.md)
+ - [Guides](guides/stt/index.md)
+ - [Examples](examples/index.md)
+ 
```

</details>

</tool-use>

---



---


Updating `mkdocs.yml` to remove the placeholder repo URL and creating a deployment guide:


---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/aman/Documents/repos/artpark/agentloop/mkdocs.yml</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/mkdocs.yml</summary>

**Chunk 1**
Lines added: 3, lines removed: 2

```diff
@@ -2,6 +2,7 @@
  site_description: Documentation for Pense - Voice Agent Testing Framework
  site_url: https://pense-docs.netlify.app
- repo_url: https://github.com/your-username/agentloop
- repo_name: pense
+ # Update repo_url and repo_name with your actual repository details
+ # repo_url: https://github.com/your-username/agentloop
+ # repo_name: pense
  
  theme:
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/DEPLOYMENT.md</summary>

**Chunk 1**
Lines added: 105, lines removed: 1

```diff
@@ -1,1 +1,105 @@
- 
+ # Deploying Documentation to Netlify
+ 
+ This guide explains how to deploy the Pense documentation to Netlify.
+ 
+ ## Prerequisites
+ 
+ - A Netlify account (free tier works)
+ - Your repository pushed to GitHub (can be private)
+ 
+ ## Deployment Steps
+ 
+ ### 1. Connect Repository to Netlify
+ 
+ 1. Go to [Netlify](https://www.netlify.com) and sign in
+ 2. Click "Add new site" → "Import an existing project"
+ 3. Connect to GitHub and select your repository
+ 4. Netlify will auto-detect the build settings from &#96;netlify.toml&#96;
+ 
+ ### 2. Verify Build Settings
+ 
+ Netlify should automatically detect:
+ - **Build command**: &#96;pip install -r requirements-docs.txt &amp;&amp; mkdocs build&#96;
+ - **Publish directory**: &#96;site&#96;
+ - **Python version**: 3.10
+ 
+ If not auto-detected, manually configure:
+ - Build command: &#96;pip install -r requirements-docs.txt &amp;&amp; mkdocs build&#96;
+ - Publish directory: &#96;site&#96;
+ - Python version: 3.10
+ 
+ ### 3. Deploy
+ 
+ 1. Click "Deploy site"
+ 2. Netlify will build and deploy your documentation
+ 3. Your site will be available at &#96;https://&lt;random-name&gt;.netlify.app&#96;
+ 
+ ### 4. Customize Site Name (Optional)
+ 
+ 1. Go to Site settings → General
+ 2. Change "Site name" to something like &#96;pense-docs&#96;
+ 3. Your site will be available at &#96;https://pense-docs.netlify.app&#96;
+ 
+ ### 5. Update mkdocs.yml
+ 
+ Update the &#96;site_url&#96; in &#96;mkdocs.yml&#96; to match your Netlify site URL:
+ 
+ &#96;&#96;&#96;yaml
+ site_url: https://your-site-name.netlify.app
+ &#96;&#96;&#96;
+ 
+ Also update &#96;repo_url&#96; and &#96;repo_name&#96; if you want GitHub links:
+ 
+ &#96;&#96;&#96;yaml
+ repo_url: https://github.com/your-username/agentloop
+ repo_name: pense
+ &#96;&#96;&#96;
+ 
+ ## Automatic Deployments
+ 
+ Netlify automatically deploys:
+ - **Production**: Every push to your main/master branch
+ - **Preview**: Every pull request gets a preview deployment
+ 
+ ## Local Testing
+ 
+ Before deploying, test locally:
+ 
+ &#96;&#96;&#96;bash
+ # Install dependencies
+ pip install -r requirements-docs.txt
+ 
+ # Serve locally
+ mkdocs serve
+ 
+ # Build locally
+ mkdocs build
+ &#96;&#96;&#96;
+ 
+ Visit &#96;http://127.0.0.1:8000&#96; to preview your documentation.
+ 
+ ## Troubleshooting
+ 
+ ### Build Fails
+ 
+ - Check Netlify build logs for errors
+ - Verify Python version is 3.10+
+ - Ensure &#96;requirements-docs.txt&#96; is in the repository
+ 
+ ### Site Not Updating
+ 
+ - Check that changes are pushed to the main branch
+ - Verify Netlify build completed successfully
+ - Clear browser cache
+ 
+ ### Missing Files
+ 
+ - Ensure all markdown files are in the &#96;docs/&#96; directory
+ - Check that &#96;mkdocs.yml&#96; navigation matches your file structure
+ 
+ ## Next Steps
+ 
+ - Customize the theme in &#96;mkdocs.yml&#96;
+ - Add more content following the documentation plan
+ - Set up a custom domain (optional)
+ 
```

</details>

</tool-use>

---



---


Set up MkDocs documentation with Netlify deployment. Here's what was created:

## Files Created

### Configuration Files
- `mkdocs.yml` - MkDocs configuration with Material theme
- `requirements-docs.txt` - Documentation dependencies
- `netlify.toml` - Netlify build configuration
- `DEPLOYMENT.md` - Deployment instructions

### Documentation Structure
- `docs/index.md` - Landing page
- `docs/getting-started/` - Installation, configuration, first evaluation
- `docs/concepts/` - Core concepts (workflow, metrics, providers, outputs)
- `docs/guides/` - Complete guides for STT, TTS, LLM, and Agent evaluation
- `docs/reference/` - CLI reference, config reference, providers, metrics
- `docs/examples/` - Quick start examples
- `docs/troubleshooting/` - Common issues and solutions
- `docs/faq.md` - Frequently asked questions

### Updated Files
- `.gitignore` - Added `site/` directory (MkDocs build output)

## Next Steps

1. Update `mkdocs.yml`:
   - Uncomment and update `repo_url` with your GitHub repository URL
   - Update `site_url` after deploying to Netlify

2. Deploy to Netlify:
   - Follow the instructions in `DEPLOYMENT.md`
   - Connect your GitHub repository
   - Netlify will auto-detect settings from `netlify.toml`

3. Test locally (optional):
   ```bash
   pip install -r requirements-docs.txt
   mkdocs serve
   ```

4. Expand content:
   - The structure is in place with placeholder content
   - Expand each section with detailed information from your README

The documentation is ready to deploy. Netlify will automatically rebuild and deploy whenever you push changes to your main branch.

---

_**User (2026-01-06 17:39Z)**_

update README at the end before TODOs on instructions for how to build the docs

---

_**Agent (model composer-1, mode Agent)**_



---


Adding documentation build instructions to the README before the TODOs section. Checking the current README structure:


---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/aman/Documents/repos/artpark/agentloop/README.md</summary>

</details>
</tool-use>

---



---


Adding documentation build instructions before the TODOs section:


---

<tool-use data-tool-type="write" data-tool-name="search_replace">

<details><summary>Tool use: **search_replace** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/README.md</summary>

**Chunk 1**
Lines added: 52, lines removed: undefined

```diff
@@ -863,4 +863,56 @@
  
  You can checkout [&#96;pense/agent/examples/simulation/sample_output&#96;](pense/agent/examples/simulation/sample_output) to see a sample output of the voice agent simulation.
+ 
+ ## Documentation
+ 
+ Pense documentation is built using [MkDocs](https://www.mkdocs.org) with the [Material theme](https://squidfunk.github.io/mkdocs-material/).
+ 
+ ### Building Documentation Locally
+ 
+ 1. **Install documentation dependencies:**
+ 
+ &#96;&#96;&#96;bash
+ pip install -r requirements-docs.txt
+ &#96;&#96;&#96;
+ 
+ 2. **Preview documentation locally:**
+ 
+ &#96;&#96;&#96;bash
+ mkdocs serve
+ &#96;&#96;&#96;
+ 
+ This will start a local server at &#96;http://127.0.0.1:8000&#96; where you can preview the documentation.
+ 
+ 3. **Build static site:**
+ 
+ &#96;&#96;&#96;bash
+ mkdocs build
+ &#96;&#96;&#96;
+ 
+ This creates a &#96;site/&#96; directory with the static HTML files.
+ 
+ ### Deploying Documentation
+ 
+ The documentation is configured for automatic deployment to Netlify (works with private repositories).
+ 
+ **To deploy:**
+ 
+ 1. Connect your GitHub repository to [Netlify](https://www.netlify.com)
+ 2. Netlify will automatically detect the build settings from &#96;netlify.toml&#96;:
+    - Build command: &#96;pip install -r requirements-docs.txt &amp;&amp; mkdocs build&#96;
+    - Publish directory: &#96;site&#96;
+ 3. Documentation will automatically rebuild and deploy on every push to the main branch
+ 
+ **Update site URL:**
+ 
+ After deployment, update the &#96;site_url&#96; in &#96;mkdocs.yml&#96; to match your Netlify site URL.
+ 
+ **Alternative deployment options:**
+ 
+ - **GitHub Pages**: Use GitHub Actions workflow (see plan for details)
+ - **Vercel**: Connect repository and configure build settings
+ - **Cloudflare Pages**: Connect repository and set build command
+ 
+ For more details, see the documentation structure in the &#96;docs/&#96; directory.
  
  ## TODOs
```

</details>

</tool-use>

---



---


Added documentation build instructions to the README before the TODOs section. The section includes:

1. **Building locally** - How to install dependencies, preview, and build the docs
2. **Deploying** - Instructions for Netlify deployment (with automatic detection)
3. **Alternative options** - Brief mention of other deployment platforms

The instructions are concise and cover the essential steps for building and deploying the documentation.

---

