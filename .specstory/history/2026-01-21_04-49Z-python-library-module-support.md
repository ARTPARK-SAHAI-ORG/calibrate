<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 01c1b0b2-468b-47f8-9b6a-f5180c8032e3 (2026-01-21 04:49Z) -->

# Python library module support (2026-01-21 04:49Z)

_**User**_

apart from cli also add support for making each of the modules be callable as a python library

e.g. instead of pense stt eval or pense stt leaderboard ... as mentioned in @pense/cli.py 

also add support for:

from pense.stt import eval, leaderboard

and do this for all the key features mentioned in @README.md 

---

_**Agent (model claude-4.5-opus-high-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
The user wants me to add Python library support for all the key features that are currently only accessible via CLI. Let me first read the CLI file and README to understand the current structure and what features need to be exposed as a Python library.</details></think>

---

I'll read the CLI file and README to understand the current structure and what features need to be exposed as Python library functions.

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/cli.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/README.md</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Now I understand the structure. The user wants me to add Python library support for all the key features that are currently CLI-only. Based on the CLI and README, here are the key features:

1. **STT (Speech-to-Text)**:
   - `eval` - Run STT evaluation
   - `leaderboard` - Generate STT leaderboard

2. **TTS (Text-to-Speech)**:
   - `eval` - Run TTS evaluation
   - `leaderboard` - Generate TTS leaderboard

3. **LLM Tests**:
   - `run` - Run LLM tests
   - `leaderboard` - Generate LLM tests leaderboard

4. **LLM Simulations**:
   - `run` - Run LLM simulations
   - `leaderboard` - Generate LLM simulation leaderboard

5. **Agent**:
   - `test` - Interactive agent test
   - `simulation` - Run agent simulation

I need to read the existing implementation files to understand the current API and then create proper library functions. Let me read the relevant implementation files.</details></think>

---

Let me read the current implementation files to understand how they work and create proper library APIs.

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/stt/eval.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/stt/leaderboard.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/tts/eval.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/tts/leaderboard.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Now let me read the LLM module files (run_tests.py, tests_leaderboard.py, run_simulation.py, simulation_leaderboard.py) and the agent module files.</details></think>

---

Let me also read the LLM and agent module files.

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/llm/run_tests.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/llm/tests_leaderboard.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/llm/run_simulation.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/llm/simulation_leaderboard.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/agent/run_simulation.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Now I have a good understanding of all the modules. Let me also read the existing `__init__.py` files to understand the current module structure.</details></think>

---

Let me also check the current `__init__.py` files to understand the existing module structure.

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/stt/__init__.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/tts/__init__.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/llm/__init__.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/agent/__init__.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/pense/__init__.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Now I understand the structure. I need to:

1. Create library functions that wrap the main functionality of each module
2. Export these functions from the `__init__.py` files

Based on my analysis:

**STT Module:**
- `eval` - from `pense/stt/eval.py` (main function + supporting functions)
- `leaderboard` - from `pense/stt/leaderboard.py` (generate_leaderboard function)

**TTS Module:**
- `eval` - from `pense/tts/eval.py` (main function + supporting functions)
- `leaderboard` - from `pense/tts/leaderboard.py` (generate_leaderboard function)

**LLM Module:**
- `tests.run` - from `pense/llm/run_tests.py` (run_test, run_inference functions)
- `tests.leaderboard` - from `pense/llm/tests_leaderboard.py` (generate_leaderboard function)
- `simulations.run` - from `pense/llm/run_simulation.py` (run_simulation function)
- `simulations.leaderboard` - from `pense/llm/simulation_leaderboard.py` (generate_leaderboard function)

**Agent Module:**
- `simulation` - from `pense/agent/run_simulation.py` (run_simulation function)
- `test` - from `pense/agent/test.py` - this is a bit more complex since it uses pipecat runner

For the library API, I need to create clean wrapper functions that don't require argparse. Looking at the existing code:

1. The `generate_leaderboard` functions in leaderboard files are already good library functions - they take parameters directly
2. The `run_*` functions have async main functions that parse args, but also have core functions like `run_simulation`, `run_test`, etc.


3. I'll create wrapper functions that provide a clean Python API by extracting the core async functions and exporting them from `__init__.py` files, starting with the implementation.</details></think>

---

Now I have a complete picture of the codebase. I'll create Python library APIs for all the key features. Let me update the `__init__.py` files to expose clean library functions.

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/pense/stt/__init__.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/pense/tts/__init__.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/pense/llm/__init__.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/pense/agent/__init__.py</summary>

</details>

</tool-use>

---

<think><details><summary>Thought Process</summary>
Now I should update the main `pense/__init__.py` to provide a convenient way to access all these modules. Let me also check if there are linter errors.</details></think>

---

Now let me update the main `pense/__init__.py` to expose all modules conveniently.

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/pense/__init__.py</summary>

</details>

</tool-use>

---

<think><details><summary>Thought Process</summary>
Let me check for any linter errors in the files I created.</details></think>

---

Let me check for linter errors in the files I modified.

---

<tool-use data-tool-type="unknown" data-tool-name="read_lints">
<details>
<summary>Tool use: **read_lints**</summary>


Parameters:

```json
{
  "paths": [
    "/Users/aman/Documents/repos/artpark/agentloop/pense/__init__.py",
    "/Users/aman/Documents/repos/artpark/agentloop/pense/stt/__init__.py",
    "/Users/aman/Documents/repos/artpark/agentloop/pense/tts/__init__.py",
    "/Users/aman/Documents/repos/artpark/agentloop/pense/llm/__init__.py",
    "/Users/aman/Documents/repos/artpark/agentloop/pense/agent/__init__.py"
  ]
}
```
Result:

```json
{
  "linterErrorsByFile": [
    {
      "relativeWorkspacePath": "/Users/aman/Documents/repos/artpark/agentloop/pense/__init__.py"
    },
    {
      "relativeWorkspacePath": "/Users/aman/Documents/repos/artpark/agentloop/pense/stt/__init__.py"
    },
    {
      "relativeWorkspacePath": "/Users/aman/Documents/repos/artpark/agentloop/pense/tts/__init__.py"
    },
    {
      "relativeWorkspacePath": "/Users/aman/Documents/repos/artpark/agentloop/pense/llm/__init__.py"
    },
    {
      "relativeWorkspacePath": "/Users/aman/Documents/repos/artpark/agentloop/pense/agent/__init__.py"
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
The linter passes. Now let me verify the imports work correctly by checking if there are any issues with the way the internal modules are being imported. Let me also look at the run_tests.py and run_simulation.py to make sure the functions I'm importing actually exist with those signatures.

Looking at the code:
- `pense/stt/eval.py` has `async def main()` - good
- `pense/stt/leaderboard.py` has `generate_leaderboard(output_dir, save_dir)` - good
- `pense/tts/eval.py` has `async def main()` - good
- `pense/tts/leaderboard.py` has `generate_leaderboard(output_dir, save_dir)` - good
- `pense/llm/run_tests.py` has `async def main()`, `run_test()`, `run_inference()` - good
- `pense/llm/tests_leaderboard.py` has `generate_leaderboard(output_dir, save_dir)` - good
- `pense/llm/run_simulation.py` has `async def main()`, `run_simulation()` - good
- `pense/llm/simulation_leaderboard.py` has `generate_leaderboard(output_dir, save_dir)` - good
- `pense/agent/run_simulation.py` has `async def main()`, `run_simulation()`, `run_single_simulation_task()` - good

The imports should work. But wait, I need to check the actual signatures of the functions I'm exposing to make sure they match what I documented. Let me verify the signatures.

For `pense/llm/run_tests.py`:
- `run_test(chat_history, evaluation, system_prompt, model, provider, tools=[])` - matches
- `run_inference(chat_history, system_prompt, model, provider, tools)` - matches


- `pense/llm/run_simulation.py` has `run_simulation(bot_system_prompt, tools, user_system_prompt, evaluation_criteria, bot_model, user_model, bot_provider, user_provider, agent_speaks_first, max_turns, output_dir)` - matches
- `pense/agent/run_simulation.py` has `run_simulation(system_prompt, language, gender, evaluation_criteria, output_dir, interrupt_probability, port, agent_speaks_first, max_turns)` - matches, and `run_single_simulation_task(config, persona_index, user_persona, scenario_index, scenario, output` - this one appears to be cut off but the signature looks correct so far

The signatures check out. I should verify whether `asyncio` is actually being used at the module level or if it's only appearing in docstring examples. Looking at `pense/stt/__init__.py` and `pense/tts/__init__.py`, I see `asyncio` imported but not utilized anywhere in the actual code, so I'll remove those unnecessary imports.</details></think>

---

Let me remove unused imports in the `__init__.py` files.

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/pense/stt/__init__.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/pense/tts/__init__.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/pense/llm/__init__.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/pense/agent/__init__.py</summary>

</details>

</tool-use>

---

<think><details><summary>Thought Process</summary>
Good, the unused imports are removed. Now let me verify the final state of the files and check for any lint errors again.</details></think>

---

Let me verify there are no linter errors after the changes.

---

<tool-use data-tool-type="unknown" data-tool-name="read_lints">
<details>
<summary>Tool use: **read_lints**</summary>


Parameters:

```json
{
  "paths": [
    "/Users/aman/Documents/repos/artpark/agentloop/pense/__init__.py",
    "/Users/aman/Documents/repos/artpark/agentloop/pense/stt/__init__.py",
    "/Users/aman/Documents/repos/artpark/agentloop/pense/tts/__init__.py",
    "/Users/aman/Documents/repos/artpark/agentloop/pense/llm/__init__.py",
    "/Users/aman/Documents/repos/artpark/agentloop/pense/agent/__init__.py"
  ]
}
```
Result:

```json
{
  "linterErrorsByFile": [
    {
      "relativeWorkspacePath": "/Users/aman/Documents/repos/artpark/agentloop/pense/__init__.py"
    },
    {
      "relativeWorkspacePath": "/Users/aman/Documents/repos/artpark/agentloop/pense/stt/__init__.py"
    },
    {
      "relativeWorkspacePath": "/Users/aman/Documents/repos/artpark/agentloop/pense/tts/__init__.py"
    },
    {
      "relativeWorkspacePath": "/Users/aman/Documents/repos/artpark/agentloop/pense/llm/__init__.py"
    },
    {
      "relativeWorkspacePath": "/Users/aman/Documents/repos/artpark/agentloop/pense/agent/__init__.py"
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
All linter errors are cleared. Let me now provide a summary to the user of what I've implemented.</details></think>

---

I've added Python library support for all key features. Here's a summary of what's now available:

## STT Module
```python
from pense.stt import eval, leaderboard

# Run STT evaluation
import asyncio
result = asyncio.run(eval(
    provider="deepgram",  # deepgram, openai, cartesia, google, sarvam, elevenlabs, etc.
    language="english",
    input_dir="./data",
    output_dir="./out"
))

# Generate leaderboard
leaderboard(output_dir="./out", save_dir="./leaderboard")
```

## TTS Module
```python
from pense.tts import eval, leaderboard

# Run TTS evaluation
import asyncio
result = asyncio.run(eval(
    provider="google",  # cartesia, openai, groq, google, elevenlabs, sarvam
    language="english",
    input="./data/sample.csv",
    output_dir="./out"
))

# Generate leaderboard
leaderboard(output_dir="./out", save_dir="./leaderboard")
```

## LLM Module
```python
from pense.llm import tests, simulations

# Run LLM tests
import asyncio
result = asyncio.run(tests.run(
    config="./config.json",
    output_dir="./out",
    model="gpt-4.1",
    provider="openrouter"
))
tests.leaderboard(output_dir="./out", save_dir="./leaderboard")

# Run LLM simulations
result = asyncio.run(simulations.run(
    config="./config.json",
    output_dir="./out",
    model="gpt-4.1",
    provider="openrouter",
    parallel=1
))
simulations.leaderboard(output_dir="./out", save_dir="./leaderboard")

# Lower-level APIs for direct control
result = await tests.run_test(chat_history, evaluation, system_prompt, model, provider, tools)
result = await tests.run_inference(chat_history, system_prompt, model, provider, tools)
result = await simulations.run_simulation(bot_system_prompt, tools, user_system_prompt, ...)
```

## Agent Module
```python
from pense.agent import simulation

# Run agent simulations from config
import asyncio
result = asyncio.run(simulation.run(
    config="./config.json",
    output_dir="./out",
    port=8765
))

# Run a single simulation with custom parameters
result = asyncio.run(simulation.run_single(
    system_prompt="You are simulating a user...",
    language="english",
    gender="female",
    evaluation_criteria=[{"name": "completeness", "description": "..."}],
    output_dir="./out"
))
```

All CLI commands continue to work as before while these new library APIs provide programmatic access to the same functionality.

---

_**User**_

update the README to use python sdk instead of the cli version



---

_**Agent (model claude-4.5-opus-high-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
The user wants me to update the README.md to show Python SDK usage instead of CLI commands. Let me read the README again and update all the CLI examples to use the Python SDK instead.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/aman/Documents/repos/artpark/agentloop/README.md</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
Now I'll update the README to use Python SDK instead of CLI commands. I need to replace all the CLI examples with Python SDK equivalents while keeping all the explanatory content and output format documentation.</details></think>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/aman/Documents/repos/artpark/agentloop/README.md</summary>

</details>

</tool-use>

---

<think><details><summary>Thought Process</summary>
The README has been updated to use Python SDK instead of CLI commands. Let me verify it by checking for any remaining CLI commands that should have been converted.</details></think>

---

I've updated the README to use the Python SDK instead of CLI commands. Here's a summary of the changes:

1. **STT Evaluation** - Converted from `pense stt eval -p <provider> ...` to:
   ```python
   from pense.stt import eval, leaderboard
   asyncio.run(eval(provider="deepgram", input_dir="./data", ...))
   leaderboard(output_dir="./out", save_dir="./leaderboard")
   ```

2. **TTS Evaluation** - Converted from `pense tts eval -p <provider> ...` to:
   ```python
   from pense.tts import eval, leaderboard
   asyncio.run(eval(provider="google", input="./sample.csv", ...))
   leaderboard(output_dir="./out", save_dir="./leaderboard")
   ```

3. **LLM Tests** - Converted from `pense llm tests run -c ...` to:
   ```python
   from pense.llm import tests
   asyncio.run(tests.run(config="./config.json", model="gpt-4.1", ...))
   tests.leaderboard(output_dir="./out", save_dir="./leaderboard")
   ```

4. **LLM Simulations** - Converted from `pense llm simulations run -c ...` to:
   ```python
   from pense.llm import simulations
   asyncio.run(simulations.run(config="./config.json", model="gpt-4.1", ...))
   simulations.leaderboard(output_dir="./out", save_dir="./leaderboard")
   ```

5. **Agent Simulations** - Converted from `pense agent simulation -c ...` to:
   ```python
   from pense.agent import simulation
   asyncio.run(simulation.run(config="./config.json", output_dir="./out"))
   ```

6. **Added parameter tables** for STT and TTS eval functions documenting all available options

7. **Added low-level API sections** for LLM tests, LLM simulations, and agent simulations showing how to run individual test cases or simulations with full control

8. **Kept CLI for interactive testing** (`pense agent test`) since it requires a browser session

---

_**User**_

update the python sdk to not use a config.json file and instead pass in the config params as direct inputs to the function 

and update REAMDE as well

---

