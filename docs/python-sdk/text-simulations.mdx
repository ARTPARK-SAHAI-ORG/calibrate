---
title: "Text Simulations"
description: "Run fully automated text-only simulations between two Text to Text models."
---

Run fully automated text-only conversations between two Text to Text models—the "agent" plus multiple users having specific personas to mimic specific scenarios.

Each simulation pairs every persona with every scenario. The runner fans out into separate folders and saves transcripts, evaluation results, plus logs for inspection.

## Running Simulations

```python
import asyncio
from pense.llm import simulations

# Define your tools
tools = [
    {
        "type": "client",
        "name": "plan_next_question",
        "description": "Plan the next question to ask",
        "parameters": [
            {
                "id": "next_unanswered_question_index",
                "type": "integer",
                "description": "Index of next question",
                "required": True
            },
            {
                "id": "questions_answered",
                "type": "array",
                "description": "List of answered question indices",
                "items": {"type": "integer"},
                "required": True
            }
        ]
    }
]

# Define personas
personas = [
    {
        "characteristics": "A shy mother named Geeta, 39 years old, gives short answers",
        "gender": "female",
        "language": "english"
    }
]

# Define scenarios
scenarios = [
    {"description": "User completes the form without any issues"},
    {"description": "User hesitates and wants to skip some questions"}
]

# Define evaluation criteria
evaluation_criteria = [
    {
        "name": "question_completeness",
        "description": "Whether all the questions in the form were covered"
    },
    {
        "name": "assistant_behavior",
        "description": "Whether the assistant asks one concise question per turn"
    }
]

# Run simulations
result = asyncio.run(simulations.run(
    system_prompt="You are a helpful nurse filling out a form...",
    tools=tools,
    personas=personas,
    scenarios=scenarios,
    evaluation_criteria=evaluation_criteria,
    output_dir="./out",
    model="openai/gpt-4.1",
    provider="openrouter",
    parallel=1,
    agent_speaks_first=True,
    max_turns=50,
))
```

**Function Parameters:**

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `system_prompt` | str | Yes | - | System prompt for the bot/agent |
| `tools` | list | Yes | - | List of tool definitions |
| `personas` | list | Yes | - | List of persona dicts with 'characteristics', 'gender', 'language' |
| `scenarios` | list | Yes | - | List of scenario dicts with 'description' |
| `evaluation_criteria` | list | Yes | - | List of criteria dicts with 'name' and 'description' |
| `output_dir` | str | No | "./out" | Output directory for results |
| `model` | str | No | "gpt-4.1" | Model name for both agent and user |
| `provider` | str | No | "openrouter" | Text to Text provider: openai or openrouter |
| `parallel` | int | No | 1 | Number of parallel simulations |
| `agent_speaks_first` | bool | No | True | Whether agent initiates conversation |
| `max_turns` | int | No | 50 | Maximum assistant turns |

**Provider Options:**

- `openai`: Use OpenAI's API directly. Model names: `gpt-4.1`, `gpt-4o`
- `openrouter`: Access multiple Text to Text providers. Model names: `openai/gpt-4.1`, `anthropic/claude-3-opus`

## Metrics

Text simulations evaluate against your defined criteria using LLM Judge:

- **Evaluation Criteria Match**: Each criterion is evaluated as True/False with reasoning
- **Aggregated Stats**: Mean, standard deviation across all simulations

<Card title="Learn more about metrics" icon="chart-bar" href="/getting-started/core-concepts#metrics">
  Detailed explanation of LLM Judge and evaluation criteria
</Card>

## Output Structure

```
/path/to/output
├── simulation_persona_1_scenario_1
│   ├── transcript.json
│   ├── evaluation_results.csv
│   ├── config.json
│   ├── logs
│   └── results.log
├── simulation_persona_1_scenario_2
│   └── ...
├── results.csv
└── metrics.json
```

### config.json

Contains the persona and scenario used for each simulation:

```json
{
  "persona": {
    "characteristics": "description of user personality, background, and behavior",
    "gender": "female",
    "language": "english"
  },
  "scenario": {
    "description": "the scenario description for this simulation"
  }
}
```

### evaluation_results.csv

Contains evaluation results for each criterion:

| name | match | reasoning |
|------|-------|-----------|
| question_completeness | True | The assistant asked for the user's full name, address, and telephone number... |
| assistant_behavior | True | The assistant asked one concise question per turn... |

### results.csv

Aggregates match scores across all simulations:

| name | question_completeness | assistant_behavior |
|------|-----------------------|--------------------|
| simulation_persona_1_scenario_1 | 1.0 | 1.0 |
| simulation_persona_1_scenario_2 | 1.0 | 1.0 |

### metrics.json

Provides summary statistics for each evaluation criterion:

```json
{
  "question_completeness": {
    "mean": 1.0,
    "std": 0,
    "values": [1.0, 1.0, 1.0]
  },
  "assistant_behavior": {
    "mean": 1.0,
    "std": 0.0,
    "values": [1.0, 1.0, 1.0]
  }
}
```

## Generating Leaderboard

After running simulations for multiple models, compile a leaderboard:

```python
from pense.llm import simulations

simulations.leaderboard(
    output_dir="/path/to/output",
    save_dir="./leaderboard"
)
```

This generates:
- `llm_leaderboard.csv`: CSV file with pass percentages by model
- `llm_leaderboard.png`: Visual comparison chart

**leaderboard.csv format:**

| model | test_config_name | overall |
|-------|------------------|---------|
| openai__gpt-4.1 | 80.0 | 80.0 |
| openai__gpt-4o | 100.0 | 100.0 |

**Function Parameters:**

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `output_dir` | str | Yes | - | Directory containing simulation results |
| `save_dir` | str | Yes | - | Directory to save leaderboard files |

## Low-level API

For more control, run a single simulation directly:

```python
import asyncio
from pense.llm import simulations

result = asyncio.run(simulations.run_simulation(
    bot_system_prompt="You are a helpful assistant...",
    tools=[...],
    user_system_prompt="You are simulating a user with persona...",
    evaluation_criteria=[
        {"name": "completeness", "description": "..."},
    ],
    bot_model="gpt-4.1",
    user_model="gpt-4.1",
    bot_provider="openrouter",
    user_provider="openrouter",
    agent_speaks_first=True,
    max_turns=50,
    output_dir="./out"
))
```
