---
title: "Text to Speech"
description: "Evaluate different Text to Speech providers on your text dataset."
---

## Input Data Structure

Prepare an input CSV file with the following structure:

| id | text |
|----|------|
| row_1 | hello world |
| row_2 | this is a test |

The CSV should have two columns: `id` (unique identifier for each text) and `text` (the text strings you want to synthesize into speech).

## Running Provider Evaluation

```python
import asyncio
from pense.tts import eval

asyncio.run(eval(
    provider="google",      # cartesia, openai, groq, google, elevenlabs, sarvam
    language="english",     # english, hindi, or kannada
    input="/path/to/input.csv",
    output_dir="/path/to/output",
    debug=True,             # optional: run on first 5 texts only
    debug_count=5,          # optional: number of texts in debug mode
))
```

**Function Parameters:**

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `input` | str | Yes | - | Path to input CSV file containing texts to synthesize |
| `provider` | str | No | "google" | Text to Speech provider: cartesia, openai, groq, google, elevenlabs, sarvam |
| `language` | str | No | "english" | Language: english, hindi, or kannada |
| `output_dir` | str | No | "./out" | Path to output directory for results |
| `debug` | bool | No | False | Run on first N texts only |
| `debug_count` | int | No | 5 | Number of texts in debug mode |
| `port` | int | No | 8765 | WebSocket port for Text to Speech bot |

## Output Structure

```
/path/to/output/provider
├── audios
│   ├── 1.wav
│   ├── 2.wav
├── logs
├── results.log
├── results.csv
└── metrics.json
```

### results.csv

Contains detailed results for each text:

| id | text | audio_path | llm_judge_score | llm_judge_reasoning |
|----|------|------------|-----------------|---------------------|
| row_1 | hello world | ./out/audios/1.wav | True | The provided audio says 'hello world'. The pronunciation is clear, and the words match exactly. |
| row_2 | this is a test | ./out/audios/2.wav | True | The audio clearly says 'this is a test' without any deviations or mispronunciations. |

### metrics.json

Contains aggregated metrics:

```json
{
  "llm_judge_score": 1.0,
  "ttfb": {
    "mean": 0.3538844585418701,
    "std": 0.026930570602416992,
    "values": [0.3808150291442871, 0.3269538879394531]
  },
  "processing_time": {
    "mean": 0.00022804737091064453,
    "std": 2.7060508728027344e-5,
    "values": [0.0002009868621826172, 0.0002551078796386719]
  }
}
```

## Metrics

Text to Speech evaluation measures both quality and latency:

**Quality metrics:**
- **LLM Judge Score**: Semantic evaluation of pronunciation accuracy

**Latency metrics:**
- **TTFB (Time to First Byte)**: Time until first audio chunk
- **Processing Time**: Total time to complete synthesis

<Card title="Learn more about metrics" icon="chart-bar" href="/getting-started/core-concepts#metrics">
  Detailed explanation of all metrics and how LLM Judge works
</Card>

## Provider Leaderboard

After running multiple provider evaluations, generate a combined leaderboard:

```python
from pense.tts import leaderboard

leaderboard(
    output_dir="/path/to/output",
    save_dir="./leaderboards"
)
```

This scans each run directory, reads `metrics.json` and `results.csv`, then generates:
- `tts_leaderboard.xlsx`: Excel file with all metrics by provider
- Individual metric charts: `llm_judge_score.png`, `ttfb.png`, `processing_time.png`

**Function Parameters:**

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `output_dir` | str | Yes | - | Directory containing provider evaluation results |
| `save_dir` | str | Yes | - | Directory to save leaderboard files |
