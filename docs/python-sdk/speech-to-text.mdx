---
title: "Speech to Text"
description: "Evaluate different Speech to Text providers on your dataset"
---

## Input data structure

Organize your input data in the following structure:

```
├── /path/to/data
│   └── stt.csv
│   └── audios/wav
│       └── audio_1.wav
│       └── audio_2.wav
│   └── audios/pcm16
│       └── audio_1.wav
│       └── audio_2.wav
```

`stt.csv` should have the following format:

| id | text |
|----|------|
| audio_1 | Hi |
| audio_2 | Madam, my name is Geeta Shankar |

Store the audios in both `wav` and `pcm16` formats as different providers support different formats. The evaluation script will automatically select the right format for each provider.

## Run evaluation

```python
import asyncio
from pense.stt import eval

asyncio.run(eval(
    provider="deepgram",  # deepgram, openai, cartesia, google, sarvam, elevenlabs, groq
    language="english",   # english, hindi, or kannada
    input_dir="/path/to/data",
    output_dir="/path/to/output",
    debug=True,           # optional: run on first 5 audio files only
    debug_count=5,        # optional: number of files in debug mode
))
```

**Function Parameters:**

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `provider` | str | Yes | - | Speech to Text provider: deepgram, openai, cartesia, google, sarvam, elevenlabs, groq |
| `input_dir` | str | Yes | - | Path to input directory containing audio files and stt.csv |
| `output_dir` | str | No | "./out" | Path to output directory for results |
| `language` | str | No | "english" | Language of audio files: english, hindi, or kannada |
| `input_file_name` | str | No | "stt.csv" | Name of input CSV file |
| `debug` | bool | No | False | Run on first N audio files only |
| `debug_count` | int | No | 5 | Number of files in debug mode |
| `ignore_retry` | bool | No | False | Skip retry if not all audios processed |
| `port` | int | No | 8765 | WebSocket port for Speech to Text bot |

## Evaluation process

When you run the evaluation, each audio file is streamed to the Speech to Text provider and the received transcript is displayed in real-time:

```
--------------------------------
Created new user transcript
Starting new audio streaming 1/13: /path/to/audio_1.wav
Finished streaming audio: /path/to/audio_1.wav
appending to last user transcript: Hi.
--------------------------------
Created new user transcript
Starting new audio streaming 2/13: /path/to/audio_2.wav
Finished streaming audio: /path/to/audio_2.wav
appending to last user transcript: Madam,
appending to last user transcript: my name is Geeta Shankar.
```

### Intermediate saves and crash recovery

After each audio file is processed, the script saves intermediate results to `results.csv`. This means:

- If the process crashes or is interrupted, you won't lose progress
- You can resume from where you left off by running the same command again

### Automatic retry logic

The evaluation script includes automatic retry logic for robustness:

1. After processing all audio files, the script checks if any transcripts are missing
2. If some audios failed to get transcripts, it automatically retries those specific files
3. If a retry attempt makes no progress (same number of failures as before), the script exits the loop and saves empty transcripts for the failed files
4. You can pass the same parameters to resume from where it left off

To skip retries and proceed directly to metrics calculation:

```python
asyncio.run(eval(
    provider="deepgram",
    input_dir="/path/to/data",
    output_dir="/path/to/output",
    ignore_retry=True,  # Skip retry logic
))
```

## Metrics

Speech to Text evaluation measures both accuracy and latency:

**Accuracy metrics:**
- **WER (Word Error Rate)**: Edit distance between predicted and reference transcripts
- **String Similarity**: Character-level similarity ratio
- **LLM Judge Score**: Semantic evaluation using an LLM

**Latency metrics:**
- **TTFB (Time to First Byte)**: Time until first transcription response
- **Processing Time**: Total time to complete transcription

<Card title="Learn more about metrics" icon="chart-bar" href="/getting-started/core-concepts#metrics">
  Detailed explanation of all metrics, including why LLM Judge is necessary
</Card>

## Output structure

```
/path/to/output/provider
├── results.csv
├── metrics.json
├── results.log
└── logs
```

### results.csv

Contains detailed results for each audio file:

| id | gt | pred | wer | string_similarity | llm_judge_score | llm_judge_reasoning |
|----|----|------|-----|-------------------|-----------------|---------------------|
| audio_1 | Hi | Hi. | 0.0 | 0.95 | True | The transcription matches the source exactly. |
| audio_2 | Please write Rekha Kumari, sister. | Please write Reha Kumari's sister. | 0.4 | 0.93 | False | The name 'Rekha' was transcribed as 'Reha', which is a different name. |

### metrics.json

Contains aggregated metrics for the entire evaluation run:

```json
[
  {
    "wer": 0.12962962962962962
  },
  {
    "string_similarity": 0.8792465033551621
  },
  {
    "llm_judge_score": 0.85
  },
  {
    "metric_name": "ttfb",
    "processor": "DeepgramSTTService#0",
    "mean": 0.487,
    "std": 0.234,
    "values": [0.486, 0.512, 0.463, ...]
  },
  {
    "metric_name": "processing_time",
    "processor": "DeepgramSTTService#0",
    "mean": 0.489,
    "std": 0.235,
    "values": [0.487, 0.514, 0.465, ...]
  }
]
```

### results.log

Contains only the terminal output shown during evaluation - the human-readable progress of audio streaming and transcript generation. This is what you see in your terminal while running the evaluation:

```
Running on port: 8765
--------------------------------
Running command: pense stt eval -p deepgram -i /path/to/data -o /path/to/output
--------------------------------
Created new user transcript
Starting new audio streaming 1/13: /path/to/audio_1.wav
Finished streaming audio: /path/to/audio_1.wav
appending to last user transcript: Hi.
--------------------------------
...
```

### logs

Contains the full debug logs of the evaluation including all pipecat pipeline logs, frame processing details, and WebSocket communication. Useful for debugging issues with specific providers or audio files.

## Generate leaderboard

After running evaluations for multiple providers, you can generate a combined leaderboard to compare them side-by-side.

### Prerequisites

For the leaderboard to work correctly, all provider evaluations must use the **same `output_dir`**. Each provider's results are saved in a subdirectory named after the provider:

```
/path/to/output/          # This is the output_dir you pass to leaderboard()
├── deepgram/
│   ├── logs
│   ├── results.log
│   ├── results.csv
│   └── metrics.json
├── google/
│   ├── logs
│   ├── results.log
│   ├── results.csv
│   └── metrics.json
├── openai/
│   ├── logs
│   ├── results.log
│   ├── results.csv
│   └── metrics.json
└── sarvam/
    ├── logs
    ├── results.log
    ├── results.csv
    └── metrics.json
```

### Generate the leaderboard

```python
from pense.stt import leaderboard

leaderboard(
    output_dir="/path/to/output",  # Same directory used for all provider evaluations
    save_dir="./leaderboards"
)
```

**Function Parameters:**

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `output_dir` | str | Yes | - | Directory containing subdirectories for each provider's evaluation results |
| `save_dir` | str | Yes | - | Directory where leaderboard files will be saved |

### Leaderboard outputs

The leaderboard generates two files:

#### stt_leaderboard.xlsx

An Excel workbook with:
- **Summary sheet**: Comparison of all providers across all metrics
- **Per-provider sheets**: Detailed results showing only the failed transcriptions (where `llm_judge_score` is False) for each provider

Example summary sheet:

| run | count | wer | string_similarity | llm_judge_score | ttfb | processing_time |
|-----|-------|-----|-------------------|-----------------|------|-----------------|
| deepgram | 50 | 0.089 | 0.934 | 0.92 | 0.487 | 0.489 |
| google | 50 | 0.112 | 0.912 | 0.88 | 0.654 | 0.656 |
| openai | 50 | 0.095 | 0.928 | 0.90 | 0.823 | 0.825 |
| sarvam | 50 | 0.156 | 0.891 | 0.82 | 2.308 | 2.309 |

#### all_metrics_by_run.png

A bar chart visualization comparing all metrics across providers:

![Leaderboard Chart](/images/stt_leaderboard_chart.png)

The chart displays:
- WER (lower is better)
- String Similarity (higher is better)
- LLM Judge Score (higher is better)
- TTFB in seconds (lower is better)
- Processing Time in seconds (lower is better)
