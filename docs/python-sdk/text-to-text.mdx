---
title: "LLM Evaluation"
description: "Evaluate LLM behavior through test cases"
---

Test your LLM's behavior by defining test cases with expected outcomes.

<Card
  title="Learn about LLM tests"
  icon="flask"
  href="/quickstart/text-to-text"
>
  Step-by-step guide to creating and running LLM tests
</Card>

## Configuration

### System Prompt

The system prompt defines your agent's behavior, personality, and capabilities. It sets the context for how the LLM should respond.

```python
system_prompt = """You are a helpful assistant filling out a medical form.
Ask questions one at a time and be empathetic with patients."""
```

**Best Practices:**

- Be specific about the agent's role and purpose
- Include any constraints or rules the agent must follow
- Specify the tone and style of communication
- Mention when tools should be called

### Tools

Tools extend your agent's capabilities by enabling structured outputs or external API calls.

<Card title="Understanding Tools" icon="wrench" href="/core-concepts/tools">
  Learn about webhook and structured output tools
</Card>

#### Structured Output Tools

These tools extract information from conversations in a defined format. Use them when you need the LLM to:

- Track conversation state
- Extract structured data from user responses
- Make decisions about conversation flow

```python
{
    "type": "client",  # or "structured_output"
    "name": "plan_next_question",
    "description": "Plan the next question to ask based on user's response",
    "parameters": [
        {
            "id": "next_unanswered_question_index",
            "type": "integer",
            "description": "Index of the next question to ask (1-indexed)",
            "required": True
        },
        {
            "id": "questions_answered",
            "type": "array",
            "description": "List of question indices that were answered in the last user response",
            "items": {"type": "integer"},
            "required": True
        }
    ]
}
```

**Key Fields:**

- `type`: Set to `"client"` or `"structured_output"` (both work the same)
- `name`: Unique identifier for the tool
- `description`: When and why the LLM should call this tool (be specific!)
- `parameters`: Array of parameter definitions

**Parameter Fields:**

- `id`: Parameter name
- `type`: Data type (`string`, `integer`, `number`, `boolean`, `array`, `object`)
- `description`: What this parameter represents and how to populate it
- `required`: Whether this field is mandatory
- `items`: For array types, defines the type of array elements

#### Webhook Tools

Webhook tools call external APIs to perform actions. Use them when you need to:

- Submit data to external systems
- Query databases or services
- Trigger notifications or workflows
- Integrate with third-party APIs

```python
{
    "type": "webhook",
    "name": "submit_form",
    "description": "Submit completed form data to the backend API",
    "webhook": {
        "method": "POST",
        "url": "https://api.example.com/forms/submit",
        "timeout": 20,  # optional, defaults to 20 seconds
        "headers": [
            {"name": "Authorization", "value": "Bearer YOUR_TOKEN"},
            {"name": "Content-Type", "value": "application/json"}
        ],
        "body": {
            "description": "Form data with user information",
            "parameters": [
                {
                    "id": "name",
                    "type": "string",
                    "description": "User's full name",
                    "required": True
                },
                {
                    "id": "age",
                    "type": "integer",
                    "description": "User's age in years",
                    "required": True
                }
            ]
        }
    }
}
```

**Webhook Configuration:**

- `method`: HTTP method (`GET`, `POST`, `PUT`, `PATCH`, `DELETE`)
- `url`: The endpoint URL to call
- `timeout`: Maximum wait time in seconds (optional, default: 20)
- `headers`: HTTP headers to include in the request
- `body.parameters`: Data fields to send in the request body

**Important Notes:**

- Webhook tools make actual HTTP calls during evaluation
- The LLM provides values for all parameters
- Failed requests return error details in the response
- Use headers for authentication tokens

### Test Cases

Test cases define conversation scenarios and expected outcomes. Each test validates specific agent behavior.

#### Test Case Structure

```python
{
    "history": [
        {"role": "assistant", "content": "Hello! What is your name?"},
        {"role": "user", "content": "Aman Dalmia"}
    ],
    "evaluation": {
        "type": "tool_call",  # or "response"
        # type-specific fields below
    },
    "settings": {"language": "english"}  # optional
}
```

**History Array:**

- Defines the conversation context leading up to the test
- Each message has a `role` (`"assistant"` or `"user"`) and `content`
- The LLM generates a response based on this history
- Can include tool call messages for complex scenarios

**Settings (Optional):**

- `language`: Specify the language for the agent's response (e.g., `"english"`, `"hindi"`)

#### Tool Call Tests

Verify that the LLM calls the correct tools with the correct parameters. Use these tests to ensure:

- Tools are invoked at the right time
- Parameters are populated correctly
- Multiple tools are called in the right sequence

```python
{
    "history": [
        {"role": "assistant", "content": "What is your name?"},
        {"role": "user", "content": "Aman Dalmia"}
    ],
    "evaluation": {
        "type": "tool_call",
        "tool_calls": [
            {
                "tool": "plan_next_question",
                "arguments": {
                    "next_unanswered_question_index": 2,
                    "questions_answered": [1]
                }
            }
        ]
    }
}
```

**Evaluation Fields:**

- `type`: Set to `"tool_call"`
- `tool_calls`: Array of expected tool invocations
- Each tool call specifies:
  - `tool`: Name of the tool that should be called
  - `arguments`: Expected parameter values (exact match required)

**How It Works:**

- The test passes if the LLM calls all specified tools with matching arguments
- Order of tool calls matters
- All specified arguments must match exactly

#### Response Tests

Verify that the LLM's text response meets specific criteria. Use these tests to validate:

- Tone and style of responses
- Content accuracy
- Adherence to guidelines
- Handling of edge cases

```python
{
    "history": [
        {"role": "assistant", "content": "What is your phone number?"},
        {"role": "user", "content": "Can I skip this question?"}
    ],
    "evaluation": {
        "type": "response",
        "criteria": "The assistant should politely allow the user to skip providing their phone number and move to the next question."
    }
}
```

**Evaluation Fields:**

- `type`: Set to `"response"`
- `criteria`: Natural language description of what makes a good response

**How It Works:**

- An LLM judge evaluates whether the response meets the criteria
- The judge provides pass/fail and reasoning
- Use specific, measurable criteria for better results
- Can test for tone, content, completeness, or any text quality

## Run Tests

Define your system prompt, tools, and test cases:

```python
import asyncio
from calibrate.llm import tests

# Define your tools
tools = [
    {
        "type": "client",
        "name": "plan_next_question",
        "description": "Plan the next question to ask",
        "parameters": [
            {
                "id": "next_unanswered_question_index",
                "type": "integer",
                "description": "Index of next question",
                "required": True
            },
            {
                "id": "questions_answered",
                "type": "array",
                "description": "List of answered question indices",
                "items": {"type": "integer"},
                "required": True
            }
        ]
    }
]

# Define your test cases
test_cases = [
    {
        "history": [
            {"role": "assistant", "content": "Hello! What is your name?"},
            {"role": "user", "content": "Aman Dalmia"}
        ],
        "evaluation": {
            "type": "tool_call",
            "tool_calls": [
                {
                    "tool": "plan_next_question",
                    "arguments": {
                        "next_unanswered_question_index": 2,
                        "questions_answered": [1]
                    }
                }
            ]
        },
        "settings": {"language": "english"}
    },
    {
        "history": [
            {"role": "assistant", "content": "What is your phone number?"},
            {"role": "user", "content": "Can I skip this question?"}
        ],
        "evaluation": {
            "type": "response",
            "criteria": "The assistant should allow the user to skip giving their phone number."
        }
    }
]

# Run tests
result = asyncio.run(tests.run(
    system_prompt="You are a helpful assistant filling out a form...",
    tools=tools,
    test_cases=test_cases,
    output_dir="./out",
    model="openai/gpt-4.1",
    provider="openrouter",
    run_name="my_test_run",
))
```

**Function Parameters:**

| Parameter       | Type | Required | Default      | Description                                                               |
| --------------- | ---- | -------- | ------------ | ------------------------------------------------------------------------- |
| `system_prompt` | str  | Yes      | -            | System prompt for the LLM                                                 |
| `tools`         | list | Yes      | -            | List of tool definitions                                                  |
| `test_cases`    | list | Yes      | -            | List of test case dicts with 'history', 'evaluation', optional 'settings' |
| `output_dir`    | str  | No       | "./out"      | Output directory for results                                              |
| `model`         | str  | No       | "gpt-4.1"    | Model name                                                                |
| `provider`      | str  | No       | "openrouter" | LLM provider: openai or openrouter                                        |
| `run_name`      | str  | No       | None         | Optional name for output folder                                           |

**Provider Options:**

- `openai`: Use OpenAI's API directly. Model names: `gpt-4.1`, `gpt-4o`
- `openrouter`: Access multiple LLM providers. Model names: `openai/gpt-4.1`, `anthropic/claude-3-opus`

## Metrics

LLM tests measure pass/fail for each test case:

- **Pass Rate**: Percentage of test cases that match expected behavior
- **Evaluation types**: Tool call matching or response criteria matching (via LLM Judge)

<Card
  title="Learn more about metrics"
  icon="chart-bar"
  href="/getting-started/core-concepts#metrics"
>
  Detailed explanation of metrics and evaluation types
</Card>

## Output Structure

```
/path/to/output/<test_config_name>/<model_name>
├── results.json
├── metrics.json
└── logs
```

### results.json

Contains detailed results for each test case:

```json
[
  {
    "output": {
      "response": "Sure, I can help you with that.",
      "tool_calls": [
        {
          "tool": "plan_next_question",
          "arguments": {
            "next_unanswered_question_index": 2,
            "questions_answered": [1]
          }
        }
      ]
    },
    "metrics": {
      "passed": true
    },
    "test_case": {
      "history": [...],
      "evaluation": {...}
    }
  }
]
```

### metrics.json

Contains summary statistics:

```json
{
  "total": 5,
  "passed": 4
}
```

## Generating Leaderboard

After running tests for multiple models, compile a leaderboard:

```python
from calibrate.llm import tests

tests.leaderboard(
    output_dir="/path/to/output",
    save_dir="./leaderboard"
)
```

This generates:

- `llm_leaderboard.csv`: CSV file with pass percentages by model
- `llm_leaderboard.png`: Visual comparison chart

**leaderboard.csv format:**

| model             | test_config_name | overall |
| ----------------- | ---------------- | ------- |
| openai\_\_gpt-4.1 | 80.0             | 80.0    |
| openai\_\_gpt-4o  | 100.0            | 100.0   |

**Function Parameters:**

| Parameter    | Type | Required | Default | Description                         |
| ------------ | ---- | -------- | ------- | ----------------------------------- |
| `output_dir` | str  | Yes      | -       | Directory containing test results   |
| `save_dir`   | str  | Yes      | -       | Directory to save leaderboard files |

## Low-level APIs

For more control, run individual test cases:

### Run a Single Test Case

```python
import asyncio
from calibrate.llm import tests

result = asyncio.run(tests.run_test(
    chat_history=[
        {"role": "assistant", "content": "Hello! What is your name?"},
        {"role": "user", "content": "Aman Dalmia"}
    ],
    evaluation={
        "type": "tool_call",
        "tool_calls": [{"tool": "plan_next_question", "arguments": {...}}]
    },
    system_prompt="You are a helpful assistant...",
    model="gpt-4.1",
    provider="openrouter",
    tools=[...]
))
```

### Run LLM Inference Without Evaluation

```python
import asyncio
from calibrate.llm import tests

result = asyncio.run(tests.run_inference(
    chat_history=[...],
    system_prompt="You are a helpful assistant...",
    model="gpt-4.1",
    provider="openrouter",
    tools=[...]
))
```
