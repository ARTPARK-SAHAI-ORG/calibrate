---
title: "LLM Evaluation"
description: "Run LLM tests to evaluate model behavior."
---

Evaluate LLM behavior through test cases.

## Running Tests

Define your system prompt, tools, and test cases to evaluate LLM behavior:

```python
import asyncio
from calibrate.llm import tests

# Define your tools (structured_output type - default)
tools = [
    {
        "type": "structured_output",  # or "client" - both work the same
        "name": "plan_next_question",
        "description": "Plan the next question to ask",
        "parameters": [
            {
                "id": "next_unanswered_question_index",
                "type": "integer",
                "description": "Index of next question",
                "required": True
            },
            {
                "id": "questions_answered",
                "type": "array",
                "description": "List of answered question indices",
                "items": {"type": "integer"},
                "required": True
            }
        ]
    }
]

# Define your test cases
test_cases = [
    {
        "history": [
            {"role": "assistant", "content": "Hello! What is your name?"},
            {"role": "user", "content": "Aman Dalmia"}
        ],
        "evaluation": {
            "type": "tool_call",
            "tool_calls": [
                {
                    "tool": "plan_next_question",
                    "arguments": {
                        "next_unanswered_question_index": 2,
                        "questions_answered": [1]
                    }
                }
            ]
        },
        "settings": {"language": "english"}
    },
    {
        "history": [
            {"role": "assistant", "content": "What is your phone number?"},
            {"role": "user", "content": "Can I skip this question?"}
        ],
        "evaluation": {
            "type": "response",
            "criteria": "The assistant should allow the user to skip giving their phone number."
        }
    }
]

# Run tests
result = asyncio.run(tests.run(
    system_prompt="You are a helpful assistant filling out a form...",
    tools=tools,
    test_cases=test_cases,
    output_dir="./out",
    model="openai/gpt-4.1",
    provider="openrouter",
    run_name="my_test_run",
))
```

## Tool Types

Calibrate supports two tool types:

### Structured Output (Default)

Use `"type": "structured_output"` (or `"client"`) for tools that extract structured data from conversations. Parameters are defined at the top level.

```python
{
    "type": "structured_output",
    "name": "plan_next_question",
    "description": "Plan the next question to ask",
    "parameters": [
        {"id": "question_index", "type": "integer", "description": "Next question", "required": True}
    ]
}
```

### Webhook

Use `"type": "webhook"` for tools that call external APIs. Parameters are nested under `webhook.queryParameters` and `webhook.body.parameters`.

```python
{
    "type": "webhook",
    "name": "submit_form",
    "description": "Submit form data to external API",
    "parameters": [],  # Empty - params defined in webhook
    "webhook": {
        "method": "POST",
        "url": "https://api.example.com/submit",
        "timeout": 20,  # Optional, defaults to 20s
        "headers": [
            {"name": "Authorization", "value": "Bearer YOUR_TOKEN"}
        ],
        "queryParameters": [
            {"id": "key", "type": "string", "description": "API key", "required": True}
        ],
        "body": {
            "description": "Request body",
            "parameters": [
                {"id": "data", "type": "string", "description": "Form data", "required": True}
            ]
        }
    }
}
```

<Note>
Webhook tools make actual HTTP calls during simulations. In LLM tests, webhook calls are logged but not executed.
</Note>

**Function Parameters:**

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `system_prompt` | str | Yes | - | System prompt for the LLM |
| `tools` | list | Yes | - | List of tool definitions |
| `test_cases` | list | Yes | - | List of test case dicts with 'history', 'evaluation', optional 'settings' |
| `output_dir` | str | No | "./out" | Output directory for results |
| `model` | str | No | "gpt-4.1" | Model name |
| `provider` | str | No | "openrouter" | LLM provider: openai or openrouter |
| `run_name` | str | No | None | Optional name for output folder |

**Provider Options:**

- `openai`: Use OpenAI's API directly. Model names: `gpt-4.1`, `gpt-4o`
- `openrouter`: Access multiple LLM providers. Model names: `openai/gpt-4.1`, `anthropic/claude-3-opus`

## Metrics

LLM tests measure pass/fail for each test case:

- **Pass Rate**: Percentage of test cases that match expected behavior
- **Evaluation types**: Tool call matching or response criteria matching (via LLM Judge)

<Card title="Learn more about metrics" icon="chart-bar" href="/getting-started/core-concepts#metrics">
  Detailed explanation of metrics and evaluation types
</Card>

## Output Structure

```
/path/to/output/<test_config_name>/<model_name>
├── results.json
├── metrics.json
└── logs
```

### results.json

Contains detailed results for each test case:

```json
[
  {
    "output": {
      "response": "Sure, I can help you with that.",
      "tool_calls": [
        {
          "tool": "plan_next_question",
          "arguments": {
            "next_unanswered_question_index": 2,
            "questions_answered": [1]
          }
        }
      ]
    },
    "metrics": {
      "passed": true
    },
    "test_case": {
      "history": [...],
      "evaluation": {...}
    }
  }
]
```

### metrics.json

Contains summary statistics:

```json
{
  "total": 5,
  "passed": 4
}
```

## Generating Leaderboard

After running tests for multiple models, compile a leaderboard:

```python
from calibrate.llm import tests

tests.leaderboard(
    output_dir="/path/to/output",
    save_dir="./leaderboard"
)
```

This generates:
- `llm_leaderboard.csv`: CSV file with pass percentages by model
- `llm_leaderboard.png`: Visual comparison chart

**leaderboard.csv format:**

| model | test_config_name | overall |
|-------|------------------|---------|
| openai__gpt-4.1 | 80.0 | 80.0 |
| openai__gpt-4o | 100.0 | 100.0 |

**Function Parameters:**

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `output_dir` | str | Yes | - | Directory containing test results |
| `save_dir` | str | Yes | - | Directory to save leaderboard files |

## Low-level APIs

For more control, run individual test cases:

### Run a Single Test Case

```python
import asyncio
from calibrate.llm import tests

result = asyncio.run(tests.run_test(
    chat_history=[
        {"role": "assistant", "content": "Hello! What is your name?"},
        {"role": "user", "content": "Aman Dalmia"}
    ],
    evaluation={
        "type": "tool_call",
        "tool_calls": [{"tool": "plan_next_question", "arguments": {...}}]
    },
    system_prompt="You are a helpful assistant...",
    model="gpt-4.1",
    provider="openrouter",
    tools=[...]
))
```

### Run LLM Inference Without Evaluation

```python
import asyncio
from calibrate.llm import tests

result = asyncio.run(tests.run_inference(
    chat_history=[...],
    system_prompt="You are a helpful assistant...",
    model="gpt-4.1",
    provider="openrouter",
    tools=[...]
))
```
