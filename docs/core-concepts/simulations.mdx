---
title: "Simulations"
description: "Understanding simulation concepts: personas, scenarios, and evaluation criteria."
---

Simulations run automated conversations to test your agent with realistic user personas and scenarios. Pense supports both **text simulations** (Text to Text only) and **voice simulations** (full Speech to Text → Text to Text → Text to Speech pipeline).

## Personas

Personas define the simulated user's characteristics:

```python
persona = {
    "characteristics": "A shy mother named Geeta, 39 years old, gives short answers",
    "gender": "female",
    "language": "english",
    "interruption_sensitivity": "medium"  # voice simulations only
}
```

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `characteristics` | string | Yes | Personality, background, and behavior description |
| `gender` | string | Yes | `male` or `female` - affects voice in voice simulations |
| `language` | string | Yes | `english`, `hindi`, `kannada`, `bengali`, `malayalam`, `marathi`, `odia`, `punjabi`, `tamil`, `telugu`, `gujarati` |
| `interruption_sensitivity` | string | No | `none`, `low`, `medium`, `high` - voice only |

### Example Personas

| Characteristics | Use Case |
|-----------------|----------|
| A shy mother who gives short answers | Testing concise responses |
| An elderly farmer who speaks slowly and asks for clarification | Testing patience and clarity |
| A busy professional who wants to complete quickly | Testing efficiency |
| A confused user who provides wrong information | Testing error handling |

## Scenarios

Scenarios define the conversation context:

```python
scenario = {
    "description": "User completes the form without any issues"
}
```

### Example Scenarios

| Scenario | Tests |
|----------|-------|
| User completes the form without any issues | Happy path |
| User hesitates and wants to skip some questions | Optional field handling |
| User provides incorrect information and needs to correct it | Error correction |
| User gets distracted and goes off-topic | Conversation management |
| User speaks with heavy accent | Speech recognition robustness |

## Evaluation Criteria

Define what success looks like for each simulation:

```python
evaluation_criteria = [
    {
        "name": "question_completeness",
        "description": "Whether all the questions in the form were covered"
    },
    {
        "name": "assistant_behavior", 
        "description": "Whether the assistant asks one concise question per turn"
    },
    {
        "name": "empathy",
        "description": "Whether the assistant shows empathy when the user hesitates"
    }
]
```

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `name` | string | Yes | Identifier for the criterion |
| `description` | string | Yes | What the LLM Judge should evaluate |

### Writing Good Criteria

**Good criteria are:**
- Specific and measurable
- Focused on one aspect
- Clear about what success looks like

**Examples:**

| Good | Bad |
|------|-----|
| "The assistant asks exactly one question per turn" | "The assistant behaves well" |
| "All required fields (name, phone, address) are collected" | "The form is complete" |
| "The assistant acknowledges when the user says they want to skip" | "The assistant is flexible" |

## Simulation Matrix

Pense runs every combination of persona × scenario:

```
3 personas × 4 scenarios = 12 simulation runs
```

Each run produces:
- Full conversation transcript
- Evaluation results for each criterion
- Audio files (voice simulations only)
- Latency metrics

## Output Files

### transcript.json

Full conversation transcript:

```json
[
  {"role": "assistant", "content": "Hello! What is your name?"},
  {"role": "user", "content": "My name is Geeta."},
  {"role": "assistant", "content": "Thank you, Geeta. What is your address?"}
]
```

### evaluation_results.csv

Per-criterion evaluation results:

| name | match | reasoning |
|------|-------|-----------|
| question_completeness | True | All required questions were asked |
| assistant_behavior | True | One question per turn |
| empathy | False | Did not acknowledge hesitation |

### results.csv

Aggregated results across all simulations:

| name | question_completeness | assistant_behavior | empathy |
|------|----------------------|-------------------|---------|
| simulation_persona_1_scenario_1 | 1.0 | 1.0 | 0.0 |
| simulation_persona_1_scenario_2 | 1.0 | 1.0 | 1.0 |

### metrics.json

Summary statistics:

```json
{
  "question_completeness": {
    "mean": 1.0,
    "std": 0.0,
    "values": [1.0, 1.0, 1.0]
  },
  "assistant_behavior": {
    "mean": 0.67,
    "std": 0.58,
    "values": [1.0, 0.0, 1.0]
  }
}
```

## Voice Simulation Extras

Voice simulations include additional outputs:

### Audio Files

- `conversation.wav` - Combined audio of entire conversation
- `audios/` folder with individual turns:
  - `0_user.wav`, `1_bot.wav`, `1_user.wav`, `2_bot.wav`, ...

### Speech to Text Evaluation

- `stt_results.csv` - Per-turn transcription accuracy
- `stt_llm_judge_score` - Overall Speech to Text accuracy

### Latency Metrics

- TTFB and processing time for each component
- Breakdown by Speech to Text, Text to Text, and Text to Speech

## Next Steps

<CardGroup cols={2}>
  <Card title="Simulations Quickstart" icon="play" href="/quickstart/simulations">
    Run your first simulation.
  </Card>
  <Card title="Speech to Text Metrics" icon="microphone" href="/core-concepts/speech-to-text">
    Learn about transcription accuracy metrics.
  </Card>
</CardGroup>
