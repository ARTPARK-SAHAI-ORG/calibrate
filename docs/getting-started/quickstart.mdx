---
title: "Quickstart"
description: "Get started with Pense in under 5 minutes."
---

This guide will help you install Pense and run your first evaluation.

## Prerequisites

- Python 3.10 or higher
- [uv](https://github.com/astral-sh/uv) package manager (recommended)

## Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/artpark/pense.git
cd pense
uv sync --frozen
```

## Configuration

Copy the example environment file and add your API keys:

```bash
cp .env.example .env
```

Edit `.env` and add the API keys for the providers you want to evaluate:

```bash
# For Deepgram STT
DEEPGRAM_API_KEY=your_key

# For Google STT/TTS
GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json

# For OpenAI
OPENAI_API_KEY=your_key

# For OpenRouter LLM
OPENROUTER_API_KEY=your_key

# For Cartesia
CARTESIA_API_KEY=your_key

# For ElevenLabs
ELEVENLABS_API_KEY=your_key

# For Sarvam
SARVAM_API_KEY=your_key
```

## Run Your First Evaluation

<Tabs>
  <Tab title="STT Evaluation">
    Evaluate a speech-to-text provider:

    ```python
    import asyncio
    from pense.stt import eval

    asyncio.run(eval(
        provider="deepgram",
        language="english",
        input_dir="pense/stt/examples/sample_input",
        output_dir="./out/stt",
        debug=True,
    ))
    ```

    This will:
    1. Process audio files from the input directory
    2. Transcribe them using the specified provider
    3. Compare against ground truth
    4. Generate metrics including WER, string similarity, and LLM judge scores
  </Tab>
  <Tab title="TTS Evaluation">
    Evaluate a text-to-speech provider:

    ```python
    import asyncio
    from pense.tts import eval

    asyncio.run(eval(
        provider="google",
        language="english",
        input="pense/tts/examples/sample.csv",
        output_dir="./out/tts",
        debug=True,
    ))
    ```

    This will:
    1. Synthesize speech from text inputs
    2. Evaluate audio quality using an LLM judge
    3. Generate latency metrics (TTFB, processing time)
  </Tab>
  <Tab title="LLM Tests">
    Run LLM behavior tests:

    ```python
    import asyncio
    from pense.llm import tests

    test_cases = [
        {
            "history": [
                {"role": "assistant", "content": "What is your name?"},
                {"role": "user", "content": "John"}
            ],
            "evaluation": {
                "type": "response",
                "criteria": "The assistant should acknowledge the name."
            }
        }
    ]

    asyncio.run(tests.run(
        system_prompt="You are a helpful assistant.",
        tools=[],
        test_cases=test_cases,
        output_dir="./out/llm",
        model="openai/gpt-4.1",
        provider="openrouter",
    ))
    ```
  </Tab>
</Tabs>

## View Results

After running an evaluation, check the output directory for:

- `results.csv` - Detailed results for each input
- `metrics.json` - Aggregated metrics
- `logs/` - Full pipeline logs

## Next Steps

<CardGroup cols={2}>
  <Card title="Core Concepts" icon="book" href="/getting-started/core-concepts">
    Understand evaluation metrics and simulation concepts.
  </Card>
  <Card title="STT Guide" icon="microphone" href="/guides/stt">
    Deep dive into STT evaluation.
  </Card>
  <Card title="LLM Simulations" icon="comments" href="/guides/llm-simulations">
    Run automated conversation simulations.
  </Card>
  <Card title="Voice Agent Simulations" icon="phone" href="/guides/voice-agent-simulations">
    Test full voice agent pipelines.
  </Card>
</CardGroup>
