---
title: "Text Simulations"
description: "Run text-only simulations via CLI"
---

Run automated conversations between an agent and simulated users. Each simulation pairs every persona with every scenario.

<Card
  title="Learn about simulations"
  icon="comments"
  href="/quickstart/simulations"
>
  Step-by-step guide to creating and running simulations
</Card>

## Configuration

Create a JSON configuration file that defines your agent, personas, scenarios, and evaluation criteria.

### System Prompt

Define your agent's role and instructions:

```json
{
  "system_prompt": "You are a helpful nurse filling out a medical form. Ask questions one at a time and be empathetic."
}
```

**Best Practices:**

- Clearly define the agent's role
- Include tool usage instructions
- Specify conversation style
- Mention constraints or rules

### Tools

Tools extend your agent's capabilities:

<Card title="Understanding Tools" icon="wrench" href="/core-concepts/tools">
  Learn about webhook and structured output tools
</Card>

**Structured output tools** track conversation state:

```json
{
  "tools": [
    {
      "type": "client",
      "name": "plan_next_question",
      "description": "Call after user answers to track progress",
      "parameters": [
        {
          "id": "next_unanswered_question_index",
          "type": "integer",
          "description": "Index of next question (1-indexed)",
          "required": true
        }
      ]
    }
  ]
}
```

**Webhook tools** call external APIs:

```json
{
  "tools": [
    {
      "type": "webhook",
      "name": "submit_form",
      "description": "Submit completed form",
      "webhook": {
        "method": "POST",
        "url": "https://api.example.com/submit",
        "headers": [{ "name": "Authorization", "value": "Bearer TOKEN" }],
        "body": {
          "parameters": [{ "id": "name", "type": "string", "required": true }]
        }
      }
    }
  ]
}
```

### Personas

Define WHO the simulated user is and HOW they behave:

<Card title="Understanding Personas" icon="user" href="/core-concepts/personas">
  Best practices for creating realistic personas
</Card>

```json
{
  "personas": [
    {
      "characteristics": "You are Geeta Prasad, a 39-year-old pregnant mother. You are shy and reserved, giving short answers unless asked for details.",
      "gender": "female",
      "language": "english"
    }
  ]
}
```

**Field Descriptions:**

- `characteristics`: Who the person is and how they behave (name, age, background, communication style, personality traits)
- `gender`: Voice gender (`"male"` or `"female"`)
- `language`: Primary language (`"english"`, `"hindi"`, etc.)

**Best Practices:**

- Be specific: Include name, age, occupation, location
- Define communication style: Formal, casual, slow, fast
- Include limitations: Language proficiency, technical knowledge
- Add emotional traits: Patient, impatient, friendly, anxious

### Scenarios

Define WHAT the simulated user should accomplish:

<Card
  title="Understanding Scenarios"
  icon="file-lines"
  href="/core-concepts/scenarios"
>
  How to write effective scenarios
</Card>

```json
{
  "scenarios": [
    {
      "description": "The user completes the form smoothly without hesitation"
    },
    {
      "description": "The user hesitates and asks to skip questions before eventually providing information"
    }
  ]
}
```

**Field Descriptions:**

- `description`: The goal, task, or situation (what happens, not who the person is)

**Best Practices:**

- Set clear objectives: What should the user accomplish?
- Include realistic context: Why is this happening?
- Add flow hints: How should the interaction unfold?
- Specify edge cases: Complications or special situations

### Evaluation Criteria

Define how to measure agent success:

```json
{
  "evaluation_criteria": [
    {
      "name": "question_completeness",
      "description": "The assistant asked all required questions: name, age, address, phone number"
    },
    {
      "name": "assistant_behavior",
      "description": "The assistant asked one concise question per turn and was empathetic"
    }
  ]
}
```

**Field Descriptions:**

- `name`: Short metric identifier
- `description`: Detailed success criteria (be specific and measurable)

**Best Practices:**

- Be specific and measurable
- Focus on one aspect per criterion
- Make it objective for the LLM judge
- Include concrete behaviors to check

### Complete Example

```json
{
  "system_prompt": "You are a helpful nurse filling out a form...",
  "tools": [
    {
      "type": "client",
      "name": "plan_next_question",
      "description": "Plan the next question",
      "parameters": [
        {
          "id": "next_unanswered_question_index",
          "type": "integer",
          "description": "Next question index",
          "required": true
        }
      ]
    }
  ],
  "personas": [
    {
      "characteristics": "A shy mother named Geeta, 39 years old, gives short answers",
      "gender": "female",
      "language": "english"
    },
    {
      "characteristics": "An elderly farmer who speaks slowly and asks for clarification",
      "gender": "male",
      "language": "hindi"
    }
  ],
  "scenarios": [
    { "description": "User completes the form without any issues" },
    { "description": "User hesitates and wants to skip some questions" }
  ],
  "evaluation_criteria": [
    {
      "name": "question_completeness",
      "description": "Whether all the questions in the form were covered"
    },
    {
      "name": "assistant_behavior",
      "description": "Whether the assistant asks one concise question per turn"
    }
  ],
  "settings": {
    "agent_speaks_first": true,
    "max_turns": 50
  }
}
```

---

## Run Simulations

Run text-only simulations with multiple personas and scenarios.

```bash
calibrate llm simulations run -c <config_file> -o <output_dir> -m <model> -p <provider> -n <parallel>
```

### Arguments

| Flag | Long           | Type   | Required | Default      | Description                                |
| ---- | -------------- | ------ | -------- | ------------ | ------------------------------------------ |
| `-c` | `--config`     | string | Yes      | -            | Path to simulation configuration JSON file |
| `-o` | `--output-dir` | string | No       | `./out`      | Path to output directory                   |
| `-m` | `--model`      | string | No       | `gpt-4.1`    | Model name for both agent and user         |
| `-p` | `--provider`   | string | No       | `openrouter` | Provider: `openai` or `openrouter`         |
| `-n` | `--parallel`   | int    | No       | `1`          | Number of simulations to run in parallel   |

### Examples

**Basic:**

```bash
calibrate llm simulations run -c ./config.json -o ./out
```

**Parallel simulations:**

```bash
calibrate llm simulations run -c ./config.json -n 4
```

**Specific model:**

```bash
calibrate llm simulations run -c ./config.json -m openai/gpt-4.1 -p openrouter
```

### Output

```
/path/to/output/
├── simulation_persona_1_scenario_1/
│   ├── transcript.json          # Full conversation
│   ├── evaluation_results.csv   # Per-criterion results
│   ├── config.json              # Persona and scenario used
│   ├── results.log
│   └── logs
├── simulation_persona_1_scenario_2/
│   └── ...
├── results.csv                  # Aggregated results
└── metrics.json                 # Summary statistics
```

**transcript.json** contains the full conversation:

```json
[
  { "role": "assistant", "content": "Hello! What is your name?" },
  { "role": "user", "content": "My name is Geeta." },
  { "role": "assistant", "content": "Thank you, Geeta. What is your address?" }
]
```

**evaluation_results.csv** contains per-criterion results:

| name                  | match | reasoning                                    |
| --------------------- | ----- | -------------------------------------------- |
| question_completeness | True  | All questions were asked and answered...     |
| assistant_behavior    | True  | The assistant asked one question per turn... |

**results.csv** aggregates match scores across all simulations:

| name                            | question_completeness | assistant_behavior |
| ------------------------------- | --------------------- | ------------------ |
| simulation_persona_1_scenario_1 | 1.0                   | 1.0                |
| simulation_persona_1_scenario_2 | 1.0                   | 0.0                |

---

## Leaderboard

Compare multiple simulation runs.

```bash
calibrate llm simulations leaderboard -o <output_dir> -s <save_dir>
```

### Arguments

| Flag | Long           | Type   | Required | Description                                 |
| ---- | -------------- | ------ | -------- | ------------------------------------------- |
| `-o` | `--output-dir` | string | Yes      | Directory containing simulation run folders |
| `-s` | `--save-dir`   | string | Yes      | Directory to save leaderboard outputs       |

### Example

```bash
calibrate llm simulations leaderboard -o ./out -s ./leaderboard
```

---

## Providers

| Provider     | Model Format   | Example                                     |
| ------------ | -------------- | ------------------------------------------- |
| `openai`     | OpenAI naming  | `gpt-4.1`, `gpt-4o`                         |
| `openrouter` | Provider/model | `openai/gpt-4.1`, `anthropic/claude-3-opus` |

## Environment Variables

```bash
# For OpenAI provider
export OPENAI_API_KEY=your_key

# For OpenRouter provider
export OPENROUTER_API_KEY=your_key
```
