---
title: "Text Simulations"
description: "CLI commands for text-only simulations between two LLMs."
---

Run text-only automated conversations between two LLMs (agent + simulated user). Each simulation pairs every persona with every scenario.

<Card
  title="Learn more about metrics"
  icon="chart-bar"
  href="/getting-started/core-concepts#metrics"
>
  Detailed explanation of LLM Judge and evaluation criteria
</Card>

---

## pense llm simulations run

Run LLM simulations from a configuration file.

```bash
pense llm simulations run -c <config_file> -o <output_dir> -m <model> -p <provider> -n <parallel>
```

### Arguments

| Flag | Long           | Type   | Required | Default      | Description                                |
| ---- | -------------- | ------ | -------- | ------------ | ------------------------------------------ |
| `-c` | `--config`     | string | Yes      | -            | Path to simulation configuration JSON file |
| `-o` | `--output-dir` | string | No       | `./out`      | Path to output directory                   |
| `-m` | `--model`      | string | No       | `gpt-4.1`    | Model name for both agent and user         |
| `-p` | `--provider`   | string | No       | `openrouter` | Provider: `openai` or `openrouter`         |
| `-n` | `--parallel`   | int    | No       | `1`          | Number of simulations to run in parallel   |

### Examples

**Basic simulation:**

```bash
pense llm simulations run -c ./config.json -o ./out
```

**Run with parallel simulations:**

```bash
pense llm simulations run -c ./config.json -o ./out -n 4
```

**Use specific model:**

```bash
pense llm simulations run -c ./config.json -o ./out -m openai/gpt-4.1 -p openrouter
```

### Configuration File Structure

```json
{
  "system_prompt": "You are a helpful nurse filling out a form...",
  "tools": [
    {
      "type": "client",
      "name": "plan_next_question",
      "description": "Plan the next question",
      "parameters": [
        {
          "id": "next_unanswered_question_index",
          "type": "integer",
          "description": "Next question index",
          "required": true
        }
      ]
    }
  ],
  "personas": [
    {
      "characteristics": "A shy mother named Geeta, 39 years old, gives short answers",
      "gender": "female",
      "language": "english"
    },
    {
      "characteristics": "An elderly farmer who speaks slowly and asks for clarification",
      "gender": "male",
      "language": "hindi"
    }
  ],
  "scenarios": [
    { "description": "User completes the form without any issues" },
    { "description": "User hesitates and wants to skip some questions" }
  ],
  "evaluation_criteria": [
    {
      "name": "question_completeness",
      "description": "Whether all the questions in the form were covered"
    },
    {
      "name": "assistant_behavior",
      "description": "Whether the assistant asks one concise question per turn"
    }
  ],
  "agent_speaks_first": true,
  "max_turns": 50
}
```

### Output Structure

```
/path/to/output/
├── simulation_persona_1_scenario_1/
│   ├── transcript.json          # Full conversation
│   ├── evaluation_results.csv   # Per-criterion results
│   ├── config.json              # Persona and scenario used
│   ├── results.log
│   └── logs
├── simulation_persona_1_scenario_2/
│   └── ...
├── results.csv                  # Aggregated results
└── metrics.json                 # Summary statistics
```

### Output Files

**transcript.json** contains the full conversation:

```json
[
  { "role": "assistant", "content": "Hello! What is your name?" },
  { "role": "user", "content": "My name is Geeta." },
  { "role": "assistant", "content": "Thank you, Geeta. What is your address?" }
]
```

**evaluation_results.csv** contains per-criterion results:

| name                  | match | reasoning                                    |
| --------------------- | ----- | -------------------------------------------- |
| question_completeness | True  | All questions were asked and answered...     |
| assistant_behavior    | True  | The assistant asked one question per turn... |

**results.csv** aggregates match scores across all simulations:

| name                            | question_completeness | assistant_behavior |
| ------------------------------- | --------------------- | ------------------ |
| simulation_persona_1_scenario_1 | 1.0                   | 1.0                |
| simulation_persona_1_scenario_2 | 1.0                   | 0.0                |

---

## pense llm simulations leaderboard

Generate a leaderboard comparing multiple simulation runs.

```bash
pense llm simulations leaderboard -o <output_dir> -s <save_dir>
```

### Arguments

| Flag | Long           | Type   | Required | Description                                 |
| ---- | -------------- | ------ | -------- | ------------------------------------------- |
| `-o` | `--output-dir` | string | Yes      | Directory containing simulation run folders |
| `-s` | `--save-dir`   | string | Yes      | Directory to save leaderboard outputs       |

### Example

```bash
pense llm simulations leaderboard -o ./out -s ./leaderboard
```

---

## Provider Options

| Provider     | Model Format   | Example                                     |
| ------------ | -------------- | ------------------------------------------- |
| `openai`     | OpenAI naming  | `gpt-4.1`, `gpt-4o`                         |
| `openrouter` | Provider/model | `openai/gpt-4.1`, `anthropic/claude-3-opus` |

## Required Environment Variables

```bash
# For OpenAI provider
export OPENAI_API_KEY=your_key

# For OpenRouter provider
export OPENROUTER_API_KEY=your_key
```
