---
title: "LLM Evaluation"
description: "Create edge cases to test the LLM powering your agent"
---

```bash
calibrate llm
```

The interactive UI guides you through the full evaluation process:

1. **Config file** — path to a JSON file containing system prompt, tools, and test cases
2. **Provider** — OpenRouter or OpenAI
3. **Model entry** — enter models one at a time (free-form text input):
   - Type the exact model name as it appears on the provider's platform
   - Examples are shown for reference (e.g., `openai/gpt-4.1` for OpenRouter)
   - After each model, choose to add another or continue
4. **Output directory** — where results will be saved (defaults to `./out`)
5. **API key setup** — enter any missing API keys
6. **Evaluation** — runs models in parallel (max 2 at a time), showing live progress in side-by-side columns
7. **Leaderboard** — displays comparison table with pass rates, bar charts, and option to drill into model details
8. **Model details** — view test-by-test results with pass/fail status and reasoning

### Output Directory

When using the interactive UI, if the output directory already contains model results or other data, you'll be prompted to confirm before overwriting:

```
⚠ Existing data found

The following directories already contain data:
  • ./out/openai__gpt-4.1
  • ./out/leaderboard

Do you want to overwrite existing results?
  > Yes, overwrite and continue
    No, enter a different path
```

### Output

Results are saved per-model with a leaderboard auto-generated after all models complete:

```
/path/to/output/
├── openai__gpt-4.1/           # Model folder (slashes replaced with __)
│   ├── results.json           # Per-test results with pass/fail
│   ├── metrics.json           # Aggregated metrics {total, passed}
│   └── results.log            # Terminal output
├── anthropic__claude-3.5-sonnet/
│   ├── results.json
│   ├── metrics.json
│   └── results.log
└── leaderboard/
    ├── llm_leaderboard.csv    # Model comparison
    └── llm_leaderboard.png    # Bar chart
```

**results.json** contains per-test evaluation results:

```json
[
  {
    "output": {
      "response": "Hello! How can I help you?",
      "tool_calls": []
    },
    "metrics": { "passed": true },
    "test_case": { ... }
  }
]
```

---

## Direct Mode

Pass all parameters as flags to skip the interactive UI:

```bash
# Single model evaluation
calibrate llm -c config.json -m gpt-4.1 -p openrouter -o ./out

# Multiple models (runs in parallel, auto-generates leaderboard)
calibrate llm -c config.json -m gpt-4.1 claude-3.5-sonnet gemini-2.0-flash -p openrouter -o ./out
```

<Note>
  When running multiple models, they execute in parallel (max 2 at a time) and a
  leaderboard is automatically generated after all models complete.
</Note>

### CLI Options

| Flag                 | Description                                  | Default      |
| -------------------- | -------------------------------------------- | ------------ |
| `-c`, `--config`     | Path to test config JSON file                | —            |
| `-m`, `--model`      | Model name(s) - space-separated for multiple | `gpt-4.1`    |
| `-p`, `--provider`   | LLM provider (`openai`/`openrouter`)         | `openrouter` |
| `-o`, `--output-dir` | Output directory                             | `./out`      |

<Note>
  For a guided setup experience with interactive prompts, run `calibrate llm`
  without any arguments.
</Note>

---

## Config File Format

The config file is a JSON file containing the system prompt, tools, and test cases:

```json
{
  "system_prompt": "You are a helpful assistant...",
  "tools": [...],
  "test_cases": [
    {
      "history": [
        {"role": "user", "content": "Hello"}
      ],
      "evaluation": {
        "type": "response",
        "criteria": "The assistant should greet the user"
      }
    }
  ]
}
```

See the [core concepts](/core-concepts/agents) documentation for details on the config structure.

## Resources

<Card title="Integrations" icon="volume-high" href="/integrations/llm">
  See the full list of supported providers and their configuration options
</Card>
